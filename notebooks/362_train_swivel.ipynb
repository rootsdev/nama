{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Train a swivel model\n",
    "\n",
    "## DEPRECATED\n",
    "\n",
    "[Swivel](https://arxiv.org/abs/1602.02215) turns out to be another key component. It's an improvement over Glove, because it trains non-matching name pairs\n",
    "to have a 0 similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import umap\n",
    "\n",
    "from nama.data.utils import load_dataset\n",
    "from nama.data.filesystem import download_file_from_s3, save_file\n",
    "from nama.eval import metrics\n",
    "from nama.models.swivel import SwivelDataset, SwivelModel, train_swivel, get_best_swivel_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Config\n",
    "\n",
    "# run this on a 256GB standard instance\n",
    "\n",
    "# TODO run both given and surname\n",
    "given_surname = \"given\"\n",
    "# given_surname = \"surname\"\n",
    "\n",
    "vocab_size = 610000 if given_surname == \"given\" else 2100000\n",
    "embed_dim = 100\n",
    "n_epochs = 200\n",
    "num_matches = 500\n",
    "Config = namedtuple(\"Config\", \"train_path eval_path vocab_size embed_dim confidence_base confidence_scale confidence_exponent n_epochs submatrix_size lr vocab_path model_path\")\n",
    "config = Config(\n",
    "    train_path=f\"s3://fs-nama-data/2024/familysearch-names/processed/tree-hr-{given_surname}-train-augmented.csv.gz\",\n",
    "    eval_path=f\"s3://fs-nama-data/2024/familysearch-names/processed/tree-hr-{given_surname}-train.csv.gz\",\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=embed_dim,\n",
    "    confidence_base=0.18 if given_surname == \"given\" else 0.14,\n",
    "    confidence_scale=0.5 if given_surname == \"given\" else 0.45,\n",
    "    confidence_exponent=0.3 if given_surname == \"given\" else 0.3,\n",
    "    lr = 0.14 if given_surname == \"given\" else 0.24,\n",
    "    n_epochs = n_epochs,\n",
    "    submatrix_size = 4096,\n",
    "    vocab_path=f\"s3://fs-nama-data/2024/nama-data/data/models/fs-{given_surname}-swivel-vocab-{vocab_size}-augmented.csv\",\n",
    "    model_path=f\"s3://fs-nama-data/2024/nama-data/data/models/fs-{given_surname}-swivel-model-{vocab_size}-{embed_dim}-augmented.pth\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "print(torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"cuda total\", torch.cuda.get_device_properties(0).total_memory)\n",
    "    print(\"cuda reserved\", torch.cuda.memory_reserved(0))\n",
    "    print(\"cuda allocated\", torch.cuda.memory_allocated(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "train_path = download_file_from_s3(config.train_path) if config.train_path.startswith(\"s3://\") else config.train_path\n",
    "input_names_train, record_name_frequencies_train, candidate_names_train = load_dataset(train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# keep only the most-frequent vocab_size names\n",
    "# input_names_train, record_name_frequencies_train, candidate_names_train = \\\n",
    "#     select_frequent_k(input_names_train, \n",
    "#                       record_name_frequencies_train, \n",
    "#                       candidate_names_train,\n",
    "#                       config.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"input_names_train\", len(input_names_train))\n",
    "print(\"record_name_frequencies_train\", sum(len(rnf) for rnf in record_name_frequencies_train))\n",
    "print(\"total pairs\", sum(freq for rnfs in record_name_frequencies_train for _, freq in rnfs))\n",
    "print(\"candidate_names_train\", len(candidate_names_train))\n",
    "print(\"total names\", len(set(input_names_train).union(set(candidate_names_train))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "dataset = SwivelDataset(input_names_train, record_name_frequencies_train, config.vocab_size, symmetric=True)\n",
    "vocab = dataset.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# get vocab names in order by id\n",
    "vocab_names = list(name_id[0] for name_id in sorted(vocab.items(), key=lambda x: x[1]))\n",
    "print(len(vocab_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "model = SwivelModel(len(vocab), config.embed_dim, config.confidence_base, config.confidence_scale, config.confidence_exponent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Initialize vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# create vectors with tfidf values\n",
    "max_ngram = 3\n",
    "min_df = 10\n",
    "max_df = 0.8\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, max_ngram), analyzer=\"char_wb\", min_df=min_df, max_df=max_df)\n",
    "tfidf_X_train = tfidf_vectorizer.fit_transform(vocab_names)\n",
    "print(tfidf_X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# reducer = TruncatedSVD(n_components=config.embed_dim)\n",
    "reducer = umap.UMAP(n_components=config.embed_dim)\n",
    "tfidf_X_train = reducer.fit_transform(tfidf_X_train)\n",
    "print(tfidf_X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# scale to uniform [-sqrt(1/embed_dim), sqrt(1/embed_dim)]\n",
    "scaler_max = math.sqrt(1 / config.embed_dim)\n",
    "scaler = MinMaxScaler(feature_range=(-scaler_max, scaler_max))\n",
    "tfidf_X_train = scaler.fit_transform(tfidf_X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# init weights to be tfidf values (does this help?)\n",
    "model.init_params(dataset.get_row_sums(), dataset.get_col_sums(), tfidf_X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import torch.optim as optim\n",
    "\n",
    "n_steps_per_epoch = 0\n",
    "\n",
    "model.to(device)\n",
    "optimizer = optim.Adagrad(model.parameters(), lr=config.lr)\n",
    "\n",
    "all_loss_values = []\n",
    "for e in tqdm(range(0, config.n_epochs)):\n",
    "    print(\"Epoch\", e, datetime.now())\n",
    "    loss_values = train_swivel(model, dataset, n_steps=n_steps_per_epoch, \n",
    "                     submatrix_size=config.submatrix_size, \n",
    "                     lr=config.lr, device=device, optimizer=optimizer)\n",
    "    all_loss_values.extend(loss_values)\n",
    "    save_file(f\"{config.model_path}.{e}\",\n",
    "              lambda local_out_path : torch.save(model.state_dict(), open(local_out_path, \"wb\")))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Save vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "vocab_df = pd.DataFrame(vocab.items(), columns=[\"name\", \"index\"])\n",
    "save_file(config.vocab_path,\n",
    "          lambda local_out_path : vocab_df.to_csv(open(local_out_path, \"wb\"), index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "save_file(config.model_path,\n",
    "          lambda local_out_path : torch.save(model.state_dict(), open(local_out_path, \"wb\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Vocab and model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Reload model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "vocab_path = download_file_from_s3(config.vocab_path) if config.vocab_path.startswith(\"s3://\") else config.vocab_path\n",
    "vocab_df = pd.read_csv(open(vocab_path, \"rb\"))\n",
    "vocab = {name: _id for name, _id in zip(vocab_df[\"name\"], vocab_df[\"index\"])}\n",
    "model = SwivelModel(len(vocab), config.embed_dim)\n",
    "model_path = download_file_from_s3(config.model_path) if config.model_path.startswith(\"s3://\") else config.model_path\n",
    "model.load_state_dict(torch.load(open(model_path, \"rb\")))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ax = plt.gca()\n",
    "ax.set_ylim([0, 1.0])\n",
    "plt.plot(all_loss_values[::1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### PR Curve"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def filter_dataset(input_names, record_name_frequencies, candidate_names):\n",
    "    input_names_filtered = []\n",
    "    record_name_frequencies_filtered = []\n",
    "    candidate_names = set()\n",
    "    for input_name, rnfs in zip(input_names, record_name_frequencies):\n",
    "        if input_name not in vocab:\n",
    "            continue\n",
    "        rnfs_filtered = []\n",
    "        max_freq = 0\n",
    "        input_name_freq = 0\n",
    "        for name, freq in rnfs:\n",
    "            if name not in vocab or freq == 0:\n",
    "                continue\n",
    "            if freq > max_freq:\n",
    "                max_freq = freq\n",
    "            if name == input_name:\n",
    "                input_name_freq = freq\n",
    "            rnfs_filtered.append((name, freq))\n",
    "        # continue if there are no associated record names, or if the input name isn't more frequently associated with itself than another name\n",
    "        if len(rnfs_filtered) == 0 or input_name_freq < max_freq:\n",
    "            continue\n",
    "        input_names_filtered.append(input_name)\n",
    "        record_name_frequencies_filtered.append(rnfs_filtered)\n",
    "        candidate_names.add(input_name)\n",
    "        candidate_names.update([name for name, _ in rnfs_filtered])\n",
    "    \n",
    "    candidate_names_filtered = np.array(list(candidate_names))\n",
    "\n",
    "    return input_names_filtered, record_name_frequencies_filtered, candidate_names_filtered"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%%time\n",
    "input_names_train_filtered, record_name_frequencies_train_filtered, candidate_names_train_filtered = \\\n",
    "    filter_dataset(input_names_train, record_name_frequencies_train, candidate_names_train)\n",
    "print(len(input_names_train_filtered))\n",
    "print(len(record_name_frequencies_train_filtered))\n",
    "print(len(candidate_names_train_filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# get best matches\n",
    "# NOTE: only considers as potential matches names in candidate_names_eval, not names in input_names_eval\n",
    "eval_batch_size = 1024\n",
    "add_context = True\n",
    "n_jobs=1\n",
    "input_names_sample = input_names_train[::10]\n",
    "record_name_frequencies_sample = record_name_frequencies_train[::10]\n",
    "best_matches = get_best_swivel_matches(model, \n",
    "                                       vocab, \n",
    "                                       input_names_sample,\n",
    "                                       candidate_names_train, \n",
    "                                       k=num_matches, \n",
    "                                       batch_size=eval_batch_size,\n",
    "                                       add_context=add_context, \n",
    "                                       n_jobs=n_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "metrics.precision_weighted_recall_curve_at_threshold(\n",
    "    record_name_frequencies_sample, best_matches, min_threshold=0.05, max_threshold=1.0, step=0.05, distances=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "metrics.get_auc(\n",
    "    record_name_frequencies_sample, best_matches, min_threshold=0.05, max_threshold=1.0, step=0.05, distances=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Eval on original (unaugmented) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "eval_path = download_file_from_s3(config.eval_path) if config.eval_path.startswith(\"s3://\") else config.eval_path\n",
    "input_names_eval, record_name_frequencies_eval, candidate_names_eval = load_dataset(eval_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "input_names_eval_filtered, record_name_frequencies_eval_filtered, candidate_names_eval_filtered = \\\n",
    "    filter_dataset(input_names_eval, record_name_frequencies_eval, candidate_names_eval)\n",
    "print(len(input_names_eval_filtered))\n",
    "print(len(record_name_frequencies_eval_filtered))\n",
    "print(len(candidate_names_eval_filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# get best matches\n",
    "# NOTE: only considers as potential matches names in candidate_names_eval, not names in input_names_eval\n",
    "eval_batch_size = 1024\n",
    "add_context = True\n",
    "n_jobs=1\n",
    "input_names_sample = input_names_eval_filtered[::10]\n",
    "record_name_frequencies_sample = record_name_frequencies_eval_filtered[::10]\n",
    "best_matches = get_best_swivel_matches(model, \n",
    "                                       vocab, \n",
    "                                       input_names_sample,\n",
    "                                       candidate_names_eval_filtered, \n",
    "                                       k=num_matches, \n",
    "                                       batch_size=eval_batch_size,\n",
    "                                       add_context=add_context, \n",
    "                                       n_jobs=n_jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### PR Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pos = 120\n",
    "input_names_sample[pos]\n",
    "# for ix, name in enumerate(input_names_sample):\n",
    "#     print(ix, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "input_names_test = input_names_sample[pos:pos+1]\n",
    "record_name_frequencies_test = record_name_frequencies_sample[pos:pos+1]\n",
    "print(record_name_frequencies_test)\n",
    "best_matches_test = best_matches[pos:pos+1]\n",
    "print(best_matches_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": "metrics.precision_at_threshold(record_name_frequencies_test[0], best_matches_test[0], 0.7725, False)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": "metrics.weighted_recall_at_threshold(record_name_frequencies_test[0], best_matches_test[0], 0.7725, False)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "metrics.precision_weighted_recall_curve_at_threshold(\n",
    "    record_name_frequencies_test, best_matches_test, min_threshold=0.05, max_threshold=1.0, step=0.05, distances=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "metrics.precision_weighted_recall_curve_at_threshold(\n",
    "    record_name_frequencies_sample, best_matches, min_threshold=0.05, max_threshold=1.0, step=0.05, distances=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "metrics.get_auc(\n",
    "    record_name_frequencies_sample, best_matches, min_threshold=0.05, max_threshold=1.0, step=0.05, distances=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": "## Review (don't run)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "threshold = 0.6\n",
    "for i in range(100001, 400000, 10000):\n",
    "    print(i, input_names_eval[i])\n",
    "    matches_above_threshold = best_matches[i][best_matches[i,:,1] > threshold]\n",
    "    matched_rnfs = []\n",
    "    unmatched_rnfs = []\n",
    "    for rnf in record_name_frequencies_eval[i]:\n",
    "        if rnf[0] in matches_above_threshold[:, 0]:\n",
    "            matched_rnfs.append(rnf)\n",
    "        elif rnf[1] > 0:\n",
    "            unmatched_rnfs.append(rnf)\n",
    "    print(\"  matched rnfs\", matched_rnfs)\n",
    "    print(\"  unmatched rnfs\", unmatched_rnfs)\n",
    "    print(\"  matches above threshold\", len(matches_above_threshold), matches_above_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "i = 390000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "best_matches[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": "record_name_frequencies_train[input_names_train.index(input_names_eval[i])]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "variant = \"<shirley>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": "record_name_frequencies_train[input_names_train.index(variant)]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": "record_name_frequencies_eval[input_names_eval.index(variant)]"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": "### Test (don't run)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nama.models.swivel import get_swivel_embeddings\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# demo names\n",
    "input_names_train = [\"<john>\", \"<mary>\"]\n",
    "record_name_frequencies_train = [\n",
    "    [(\"<johnny>\", 20), (\"<jonathan>\", 50), (\"<jon>\", 30)],\n",
    "    [(\"<marie>\", 70), (\"<maria>\", 30)],\n",
    "]\n",
    "candidate_names_train = np.array([\"<johnny>\", \"<jonathan>\", \"<marie>\", \"<maria>\", \"<jon>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "symmetric = True\n",
    "dataset = SwivelDataset(input_names_train, record_name_frequencies_train, config.vocab_size, symmetric=symmetric)\n",
    "vocab = dataset.get_vocab()\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(dataset._sparse_cooc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# get vocab names in order by id\n",
    "vocab_names = list(name_id[0] for name_id in sorted(vocab.items(), key=lambda x: x[1]))\n",
    "print(vocab_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# create vectors with tfidf values\n",
    "max_ngram = 5  # 3\n",
    "min_df = 1  # 10\n",
    "max_df = 1.0  # 0.5\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, max_ngram), analyzer=\"char_wb\", min_df=min_df, max_df=max_df)\n",
    "tfidf_X_train = tfidf_vectorizer.fit_transform(vocab_names)\n",
    "print(tfidf_X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "embed_dim = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# reduce tfidf values to embed_dim\n",
    "svd = TruncatedSVD(n_components=embed_dim)\n",
    "tfidf_X_train = svd.fit_transform(tfidf_X_train)\n",
    "tfidf_X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# create swivel model\n",
    "model = SwivelModel(len(vocab), embed_dim, config.confidence_base, config.confidence_scale, config.confidence_exponent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# init weights to tfidf values\n",
    "# model.init_params(dataset.get_row_sums(), dataset.get_col_sums(), tfidf_X_train)\n",
    "model.init_params(dataset.get_row_sums(), dataset.get_col_sums(), tfidf_X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# device=\"cpu\"\n",
    "n_steps = 10\n",
    "submatrix_size = 64\n",
    "learning_rate = 0.05\n",
    "loss_values = train_swivel(model, dataset, n_steps=n_steps, submatrix_size=submatrix_size, lr=learning_rate, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ax = plt.gca()\n",
    "# ax.set_ylim([0, 0.1])\n",
    "plt.plot(loss_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "k = 10\n",
    "add_context = True\n",
    "\n",
    "all_names = np.array(input_names_train + candidate_names_train.tolist())\n",
    "all_embeddings = get_swivel_embeddings(model, vocab, all_names, add_context=add_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(all_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "demo_name = '<john>'\n",
    "demo_name_pos = 0\n",
    "demo_embeddings = get_swivel_embeddings(model, vocab, [demo_name], add_context=add_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# try cosine similarity\n",
    "# totals = all_embeddings.sum(axis=0)\n",
    "# all_embeddings_norm = all_embeddings / totals\n",
    "# demo_embeddings_norm = all_embeddings_norm[[demo_name_pos]]\n",
    "# scores = cosine_similarity(demo_embeddings_norm, all_embeddings_norm)\n",
    "# ixs = np.argsort(-scores)[:, :k]\n",
    "# sorted_scores = scores[:, ixs[0]]\n",
    "# sorted_names = all_names[ixs[0]]\n",
    "# best_matches = np.dstack((sorted_names, sorted_scores))\n",
    "# print(\"cosine_norm_0\", best_matches)\n",
    "\n",
    "# totals = demo_embeddings.sum(axis=1)\n",
    "# demo_embeddings_norm = demo_embeddings / totals[:, np.newaxis]\n",
    "# totals = all_embeddings.sum(axis=1)\n",
    "# all_embeddings_norm = all_embeddings / totals[:, np.newaxis]\n",
    "# scores = cosine_similarity(demo_embeddings_norm, all_embeddings_norm)\n",
    "# ixs = np.argsort(-scores)[:, :k]\n",
    "# sorted_scores = scores[:, ixs[0]]\n",
    "# sorted_names = all_names[ixs[0]]\n",
    "# best_matches = np.dstack((sorted_names, sorted_scores))\n",
    "# print(\"cosine_norm_1\", best_matches)\n",
    "\n",
    "scores = cosine_similarity(demo_embeddings, all_embeddings)\n",
    "ixs = np.argsort(-scores)[:, :k]\n",
    "sorted_scores = scores[:, ixs[0]]\n",
    "sorted_names = all_names[ixs[0]]\n",
    "best_matches = np.dstack((sorted_names, sorted_scores))\n",
    "print(\"cosine\", best_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# try euclidean similarity\n",
    "totals = all_embeddings.sum(axis=0)\n",
    "all_embeddings_norm = all_embeddings / totals\n",
    "demo_embeddings_norm = all_embeddings_norm[[demo_name_pos]]\n",
    "scores = euclidean_distances(demo_embeddings_norm, all_embeddings_norm)\n",
    "ixs = np.argsort(scores)[:, :k]\n",
    "sorted_scores = scores[:, ixs[0]]\n",
    "sorted_names = all_names[ixs[0]]\n",
    "best_matches = np.dstack((sorted_names, sorted_scores))\n",
    "print(\"euclidean_norm_0\", best_matches)\n",
    "\n",
    "# totals = demo_embeddings.sum(axis=1)\n",
    "# demo_embeddings_norm = demo_embeddings / totals[:, np.newaxis]\n",
    "# totals = all_embeddings.sum(axis=1)\n",
    "# all_embeddings_norm = all_embeddings / totals[:, np.newaxis]\n",
    "# scores = euclidean_distances(demo_embeddings_norm, all_embeddings_norm)\n",
    "# ixs = np.argsort(scores)[:, :k]\n",
    "# sorted_scores = scores[:, ixs[0]]\n",
    "# sorted_names = all_names[ixs[0]]\n",
    "# best_matches = np.dstack((sorted_names, sorted_scores))\n",
    "# print(\"euclidean_norm_1\", best_matches)\n",
    "\n",
    "scores = euclidean_distances(demo_embeddings, all_embeddings)\n",
    "ixs = np.argsort(scores)[:, :k]\n",
    "sorted_scores = scores[:, ixs[0]]\n",
    "sorted_names = all_names[ixs[0]]\n",
    "best_matches = np.dstack((sorted_names, sorted_scores))\n",
    "print(\"euclidean\", best_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# plot embeddings\n",
    "xs = list(x for x, _ in all_embeddings)\n",
    "ys = list(y for _, y in all_embeddings)\n",
    "plt.scatter(xs, ys)\n",
    "for ix, name in enumerate(all_names):\n",
    "    plt.annotate(name, xy=(xs[ix], ys[ix]), xytext=(5, 2),\n",
    "                 textcoords='offset points', ha='right', va='bottom')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "source_names = np.array([\"tom\", \"dick\", \"harry\"])\n",
    "source_names_X = np.array([[1,2,3],[4,5,6],[7,8,9]])\n",
    "rows = np.array([[1,2,3],[4,5,6],[7,8,9]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "scores = cosine_similarity(rows, source_names_X)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sorted_scores_idx = np.argsort(scores, axis=1)\n",
    "sorted_scores_idx = np.flip(sorted_scores_idx, axis=1)\n",
    "sorted_scores_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sorted_scores_idx = sorted_scores_idx[:, :2]\n",
    "sorted_scores_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sorted_scores = np.take_along_axis(scores, sorted_scores_idx, axis=1)\n",
    "sorted_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sorted_source_names_X = source_names_X[sorted_scores_idx]\n",
    "sorted_source_names_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for i, (row, source_names_X) in enumerate(zip(rows, sorted_source_names_X)):\n",
    "    for j, source_name_X in enumerate(source_names_X):\n",
    "        if np.array_equal(row, source_name_X):\n",
    "            sorted_scores[i, j] = 0\n",
    "sorted_scores                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "re_sorted_scores_idx = np.argsort(sorted_scores, axis=1)\n",
    "re_sorted_scores_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "re_sorted_scores_idx = np.flip(re_sorted_scores_idx, axis=1)\n",
    "re_sorted_scores_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "re_sorted_scores_idx = re_sorted_scores_idx[:, :1]\n",
    "re_sorted_scores_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sorted_scores = np.take_along_axis(sorted_scores, re_sorted_scores_idx, axis=1)\n",
    "sorted_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sorted_scores_idx = np.take_along_axis(sorted_scores_idx, re_sorted_scores_idx, axis=1)\n",
    "sorted_scores_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sorted_source_names = source_names[sorted_scores_idx]\n",
    "sorted_source_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nama",
   "language": "python",
   "name": "nama"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
