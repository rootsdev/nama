{
 "cells": [
  {
   "cell_type": "code",
   "id": "139bee68",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T21:26:40.348650Z",
     "start_time": "2024-12-12T21:26:40.290793Z"
    }
   },
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "028823c5",
   "metadata": {},
   "source": [
    "# Train a bi-encoder\n",
    "\n",
    "learn name-to-vec encodings\n",
    "\n",
    "Hyperparameters\n",
    "* epochs\n",
    "* embedding_dim\n",
    "* hard_negs\n",
    "* easy_negs\n",
    "* bi_encoder_vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4d9cdc",
   "metadata": {},
   "source": [
    "### Results from swivel\n",
    "\n",
    "- lr = 0.001 num_epochs = 1 embedding_dim = 256 hard_negs = 5 easy_negs = 5 vocab_size = 256 Epoch 0 loss 0.024382922810735477\n",
    "- lr = 0.00005 num_epochs = 1 embedding_dim = 256 hard_negs = 15 easy_negs = 15 vocab_size = 256 Epoch 0 loss 0.023236593952613904\n",
    "- lr = 0.00005 num_epochs = 1 embedding_dim = 256 hard_negs = 10 easy_negs = 30 vocab_size = 256 Epoch 0 loss 0.025198661789189367\n",
    "- lr = 0.000025 num_epochs = 1 embedding_dim = 256 hard_negs = 10 easy_negs = 30 vocab_size = 256 Epoch 0 loss 0.025274253689720905\n",
    "- lr = 0.001 num_epochs = 1 embedding_dim = 256 hard_negs = 5 easy_negs = 5 vocab_size = 2048 Epoch 0 loss 0.020269810817918666\n",
    "- lr = 0.001 num_epochs = 1 embedding_dim = 256 hard_negs = 10 easy_negs = 20 vocab_size = 2048 Epoch 0 loss 0.021378852258816573\n",
    "- lr = 0.0001 num_epochs = 1 embedding_dim = 256 hard_negs = 10 easy_negs = 20 vocab_size = 2048 Epoch 0 loss 0.019285232987143144\n",
    "- lr = 0.00005 num_epochs = 1 embedding_dim = 256 hard_negs = 10 easy_negs = 20 vocab_size = 2048 Epoch 0 loss 0.018867752840737512\n",
    "- lr = 0.0001 num_epochs = 1 embedding_dim = 256 hard_negs = 15 easy_negs = 15 vocab_size = 2048 Epoch 0 loss 0.018372964499742593\n",
    "- lr = 0.00005 num_epochs = 1 embedding_dim = 256 hard_negs = 15 easy_negs = 15 vocab_size = 2048 Epoch 0 loss 0.01798968744305818\n",
    "- lr = 0.00005 num_epochs = 1 embedding_dim = 256 hard_negs = 10 easy_negs = 30 vocab_size = 256\n",
    "- lr = 0.000025 num_epochs = 1 embedding_dim = 256 hard_negs = 10 easy_negs = 30 vocab_size = 256 Epoch 0 loss 0.019587983736692644\n",
    "\n",
    "### Results from cross-encoder\n",
    "\n",
    "- lr = 0.00005 num_epochs = 8 embedding_dim = 256 vocab_size = 2048 Epoch 7 loss 0.02193056223032252\n",
    "- lr = 0.0001 num_epochs = 8 embedding_dim = 256 vocab_size = 2048 Epoch 7 loss 0.022243591166723677\n",
    "- lr = 0.001 num_epochs = 3 embedding_dim = 256 vocab_size = 2048 Epoch 2 loss 0.02397067276433419"
   ]
  },
  {
   "cell_type": "code",
   "id": "e0364890",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T21:26:43.523103Z",
     "start_time": "2024-12-12T21:26:40.378152Z"
    }
   },
   "source": [
    "import csv\n",
    "import gzip\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from nama.data.filesystem import download_file_from_s3, upload_file_to_s3\n",
    "from nama.models.biencoder import BiEncoder\n",
    "\n",
    "from nama.models.tokenizer import get_tokenize_function_and_vocab"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "774de195",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T21:26:43.555829Z",
     "start_time": "2024-12-12T21:26:43.525007Z"
    }
   },
   "source": [
    "# Config\n",
    "\n",
    "# TODO run both given and surname\n",
    "given_surname = \"given\"\n",
    "# given_surname = \"surname\"\n",
    "\n",
    "use_ce = True\n",
    "tree_name_min_freq = 1000\n",
    "\n",
    "# hyperparameters\n",
    "num_epochs = 12\n",
    "embedding_dim = 256\n",
    "hard_negs = 10  # only for swivel\n",
    "easy_negs = 30  # only for swivel\n",
    "bi_encoder_vocab_size = 2048\n",
    "learning_rate = 0.00005\n",
    "\n",
    "batch_size = 64\n",
    "use_amsgrad = False\n",
    "\n",
    "report_size = 10000\n",
    "max_tokens = 10\n",
    "\n",
    "tokenizer_path=f\"s3://fs-nama-data/2024/nama-data/data/models/fs-{given_surname}-subword-tokenizer-{bi_encoder_vocab_size}.json\"\n",
    "triplets_paths=[\n",
    "    # swivel path\n",
    "    # f\"s3://fs-nama-data/2024/familysearch-names/processed/tree-hr-{given_surname}-triplets-{hard_negs}-{easy_negs}.csv.gz\",\n",
    "    f\"s3://fs-nama-data/2024/familysearch-names/processed/cross-encoder-triplets-{given_surname}-train.csv\",\n",
    "    f\"s3://fs-nama-data/2024/familysearch-names/processed/cross-encoder-triplets-{given_surname}-common.csv\",\n",
    "    f\"s3://fs-nama-data/2024/familysearch-names/processed/tree-hr-{given_surname}-triplets-{tree_name_min_freq}-augmented.csv.gz\",\n",
    "]\n",
    "\n",
    "if use_ce:\n",
    "    model_filename = f\"bi_encoder-ce-{given_surname}-{num_epochs}-{embedding_dim}-{num_epochs}-{bi_encoder_vocab_size}-{learning_rate}\"\n",
    "else:\n",
    "    model_filename = f\"bi_encoder-{given_surname}-{num_epochs}-{embedding_dim}-{hard_negs}-{easy_negs}-{bi_encoder_vocab_size}-{learning_rate}\"\n",
    "    \n",
    "local_model_path_prefix = f\"../data/{model_filename}\"\n",
    "remote_model_path = f\"s3://fs-nama-data/2024/nama-data/data/models/{model_filename}.pth\""
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "34474d97",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T21:26:43.637478Z",
     "start_time": "2024-12-12T21:26:43.557334Z"
    }
   },
   "source": [
    "torch.cuda.empty_cache()\n",
    "print(torch.cuda.is_available())\n",
    "print(\"cuda total\", torch.cuda.get_device_properties(0).total_memory)\n",
    "print(\"cuda reserved\", torch.cuda.memory_reserved(0))\n",
    "print(\"cuda allocated\", torch.cuda.memory_allocated(0))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "cuda total 8141471744\n",
      "cuda reserved 0\n",
      "cuda allocated 0\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "5cb453ba",
   "metadata": {},
   "source": "## Load data"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T21:26:44.434583Z",
     "start_time": "2024-12-12T21:26:43.639617Z"
    }
   },
   "cell_type": "code",
   "source": [
    "path = download_file_from_s3(tokenizer_path) if tokenizer_path.startswith(\"s3://\") else tokenizer_path\n",
    "tokenize, tokenizer_vocab = get_tokenize_function_and_vocab(tokenizer_path=path, max_tokens=max_tokens)\n",
    "print(len(tokenizer_vocab))\n",
    "print(tokenize('dallan'))"
   ],
   "id": "a83026f57035dc3d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2048\n",
      "[1584, 489, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "adbe8f6b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T21:33:01.444460Z",
     "start_time": "2024-12-12T21:26:44.436384Z"
    }
   },
   "source": [
    "%%time\n",
    "all_data = []\n",
    "for triplets_path in triplets_paths:\n",
    "    print(triplets_path)\n",
    "    path = download_file_from_s3(triplets_path) if triplets_path.startswith(\"s3://\") else triplets_path\n",
    "    my_open = gzip.open if path.endswith(\".gz\") else open\n",
    "    with my_open(path, 'rt') as f:\n",
    "        csv_reader = csv.DictReader(f)\n",
    "        for row in tqdm(csv_reader, mininterval=10):\n",
    "            anchor_tokens = tokenize(row['anchor'])\n",
    "            pos_tokens = tokenize(row['positive'])\n",
    "            neg_tokens = tokenize(row['negative'])\n",
    "            target_margin = float(row['positive_score']) - float(row['negative_score'])\n",
    "            all_data.append((\n",
    "                anchor_tokens,\n",
    "                pos_tokens,\n",
    "                neg_tokens,\n",
    "                target_margin,\n",
    "            ))\n",
    "print(len(all_data))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://fs-nama-data/2024/familysearch-names/processed/cross-encoder-triplets-given-train.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0it [00:00, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a089359e170d482e8e35900e1af70cd8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://fs-nama-data/2024/familysearch-names/processed/cross-encoder-triplets-given-common.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0it [00:00, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a480b3dc189a478a87f259ba6f554c9a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://fs-nama-data/2024/familysearch-names/processed/tree-hr-given-triplets-1000-augmented.csv.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0it [00:00, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ac6debdc2c0a42f3909a4497d1b00530"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35486190\n",
      "CPU times: user 5min 23s, sys: 10.9 s, total: 5min 34s\n",
      "Wall time: 6min 16s\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T21:33:31.803222Z",
     "start_time": "2024-12-12T21:33:01.446125Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%time\n",
    "random.shuffle(all_data)"
   ],
   "id": "3b07131de3cac6d2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 30.3 s, sys: 0 ns, total: 30.3 s\n",
      "Wall time: 30.3 s\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T21:33:52.406306Z",
     "start_time": "2024-12-12T21:33:31.804654Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%time\n",
    "train_data, val_data = train_test_split(all_data, test_size=0.01, random_state=42)\n",
    "del all_data\n",
    "print(len(train_data), len(val_data))"
   ],
   "id": "a4e301f2670fba2f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35131328 354862\n",
      "CPU times: user 20 s, sys: 377 ms, total: 20.4 s\n",
      "Wall time: 20.6 s\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "26b5e413",
   "metadata": {},
   "source": [
    "## Train bi-encoder"
   ]
  },
  {
   "cell_type": "code",
   "id": "cbecdb9f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T21:33:52.444995Z",
     "start_time": "2024-12-12T21:33:52.407543Z"
    }
   },
   "source": [
    "def loss_fn(anchors, positives, negatives, labels):\n",
    "    # anchor_pos_sim = (anchors * positives).sum(dim=-1)\n",
    "    # anchor_neg_sim = (anchors * negatives).sum(dim=-1)\n",
    "    anchor_pos_sim = F.cosine_similarity(anchors, positives, dim=-1)\n",
    "    anchor_neg_sim = F.cosine_similarity(anchors, negatives, dim=-1)\n",
    "    margin_pred = anchor_pos_sim - anchor_neg_sim\n",
    "    return F.mse_loss(margin_pred, labels)"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "125b6c3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T21:33:55.975397Z",
     "start_time": "2024-12-12T21:33:52.446338Z"
    }
   },
   "source": [
    "# Training loop\n",
    "def train(model, train_loader, val_loader, loss_fn, optimizer, num_epochs, model_path, verbose=True):\n",
    "    for epoch in range(num_epochs):\n",
    "        # make sure gradient tracking is on\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "\n",
    "        for ix, data in enumerate(train_loader):\n",
    "            # get batch\n",
    "            anchors, positives, negatives, target_margins = data\n",
    "\n",
    "            # zero gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            anchor_embeddings = model(anchors)  # Shape: (batch_size, embedding_dim)\n",
    "            pos_embeddings = model(positives)  # Shape: (batch_size, embedding_dim)\n",
    "            neg_embeddings = model(negatives)  # Shape: (batch_size, embedding_dim)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = loss_fn(anchor_embeddings, pos_embeddings, neg_embeddings, target_margins)\n",
    "\n",
    "            # Backward pass and optimization step\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Calculate loss and report\n",
    "            if verbose:\n",
    "                running_loss += loss.item()\n",
    "                if ix % report_size == report_size - 1:\n",
    "                    avg_loss = running_loss / report_size  # loss per batch\n",
    "                    print(f\"Epoch {epoch} batch {ix} loss {avg_loss}\")\n",
    "                    running_loss = 0\n",
    "\n",
    "        # set model to evaluation mode\n",
    "        model.eval()\n",
    "\n",
    "        # disable gradient computation\n",
    "        running_loss = 0\n",
    "        num_val_batches = 0\n",
    "        with torch.no_grad():\n",
    "            for data in val_loader:\n",
    "                anchors, positives, negatives, target_margins = data\n",
    "                anchor_embeddings = model(anchors)  # Shape: (batch_size, embedding_dim)\n",
    "                pos_embeddings = model(positives)  # Shape: (batch_size, embedding_dim)\n",
    "                neg_embeddings = model(negatives)  # Shape: (batch_size, embedding_dim)\n",
    "                loss = loss_fn(anchor_embeddings, pos_embeddings, neg_embeddings, target_margins)\n",
    "                running_loss += loss.item()  \n",
    "                num_val_batches += 1\n",
    "\n",
    "        # calculate average validation loss\n",
    "        val_loss = running_loss / num_val_batches\n",
    "        if verbose:\n",
    "            print(f\"VALIDATION: Epoch {epoch} loss {val_loss}\")\n",
    "        # save model state + model\n",
    "        epoch_model_path = f\"{model_path}-{epoch}\"\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "        }, epoch_model_path+\".state\")\n",
    "        torch.save(model, epoch_model_path+\".pth\")\n",
    "        \n",
    "    # return final epoch validation loss\n",
    "    return val_loss"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "ed0b81bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T21:34:00.877071Z",
     "start_time": "2024-12-12T21:33:55.978253Z"
    }
   },
   "source": [
    "# Create an instance of the bi-encoder model\n",
    "model = BiEncoder(embedding_dim, len(tokenizer_vocab), max_tokens, tokenizer_vocab['[PAD]'])\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate, amsgrad=use_amsgrad)"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "f926160f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T21:34:00.916915Z",
     "start_time": "2024-12-12T21:34:00.878628Z"
    }
   },
   "source": [
    "def my_collate_fn(batch):\n",
    "    # Transpose the batch (list of tuples) to a tuple of lists\n",
    "    transposed_batch = list(zip(*batch))\n",
    "    # Convert each list in the tuple to a tensor\n",
    "    tensor_batch = tuple(torch.tensor(x) for x in transposed_batch)\n",
    "    return tensor_batch"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "276d6206",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T11:51:03.633128Z",
     "start_time": "2024-12-12T21:34:00.918988Z"
    }
   },
   "source": [
    "%%time\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=my_collate_fn)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True, collate_fn=my_collate_fn)\n",
    "train(model, train_loader, val_loader, loss_fn, optimizer, num_epochs, local_model_path_prefix)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 batch 9999 loss 0.12174480192176998\n",
      "Epoch 0 batch 19999 loss 0.1157549730822444\n",
      "Epoch 0 batch 29999 loss 0.1099530340641737\n",
      "Epoch 0 batch 39999 loss 0.10535381418615579\n",
      "Epoch 0 batch 49999 loss 0.10123585356995464\n",
      "Epoch 0 batch 59999 loss 0.09787476022019982\n",
      "Epoch 0 batch 69999 loss 0.09467543686442077\n",
      "Epoch 0 batch 79999 loss 0.0913670886632055\n",
      "Epoch 0 batch 89999 loss 0.0887442516848445\n",
      "Epoch 0 batch 99999 loss 0.08627734028156847\n",
      "Epoch 0 batch 109999 loss 0.08357494121044874\n",
      "Epoch 0 batch 119999 loss 0.0814166857328266\n",
      "Epoch 0 batch 129999 loss 0.07909530360940843\n",
      "Epoch 0 batch 139999 loss 0.07649908151868731\n",
      "Epoch 0 batch 149999 loss 0.07457070492338388\n",
      "Epoch 0 batch 159999 loss 0.07250337218064815\n",
      "Epoch 0 batch 169999 loss 0.0707965960489586\n",
      "Epoch 0 batch 179999 loss 0.06873922563754022\n",
      "Epoch 0 batch 189999 loss 0.06668035651966929\n",
      "Epoch 0 batch 199999 loss 0.06527890753187239\n",
      "Epoch 0 batch 209999 loss 0.06361296641919761\n",
      "Epoch 0 batch 219999 loss 0.06206824116874486\n",
      "Epoch 0 batch 229999 loss 0.06055547835547477\n",
      "Epoch 0 batch 239999 loss 0.05885264021381736\n",
      "Epoch 0 batch 249999 loss 0.05784474730025977\n",
      "Epoch 0 batch 259999 loss 0.05643054342716932\n",
      "Epoch 0 batch 269999 loss 0.05519864699225873\n",
      "Epoch 0 batch 279999 loss 0.054053585571236906\n",
      "Epoch 0 batch 289999 loss 0.0527637908404693\n",
      "Epoch 0 batch 299999 loss 0.051577396246790884\n",
      "Epoch 0 batch 309999 loss 0.05053420019634068\n",
      "Epoch 0 batch 319999 loss 0.04962579789180309\n",
      "Epoch 0 batch 329999 loss 0.04855896869432181\n",
      "Epoch 0 batch 339999 loss 0.047569530751742424\n",
      "Epoch 0 batch 349999 loss 0.04677820562720299\n",
      "Epoch 0 batch 359999 loss 0.04620610641129315\n",
      "Epoch 0 batch 369999 loss 0.04513618762213737\n",
      "Epoch 0 batch 379999 loss 0.04445005216877908\n",
      "Epoch 0 batch 389999 loss 0.04363255979139358\n",
      "Epoch 0 batch 399999 loss 0.04298750538285822\n",
      "Epoch 0 batch 409999 loss 0.042004810389876364\n",
      "Epoch 0 batch 419999 loss 0.04155158910444006\n",
      "Epoch 0 batch 429999 loss 0.04077446564380079\n",
      "Epoch 0 batch 439999 loss 0.04033964868998155\n",
      "Epoch 0 batch 449999 loss 0.039746701330319045\n",
      "Epoch 0 batch 459999 loss 0.03918413775674999\n",
      "Epoch 0 batch 469999 loss 0.03877716991333291\n",
      "Epoch 0 batch 479999 loss 0.03834086244283244\n",
      "Epoch 0 batch 489999 loss 0.03773064524279907\n",
      "Epoch 0 batch 499999 loss 0.03740142360152677\n",
      "Epoch 0 batch 509999 loss 0.0367636176337488\n",
      "Epoch 0 batch 519999 loss 0.036407183015160265\n",
      "Epoch 0 batch 529999 loss 0.036015758977178486\n",
      "Epoch 0 batch 539999 loss 0.03553696635020897\n",
      "VALIDATION: Epoch 0 loss 0.03514069266637453\n",
      "Epoch 1 batch 9999 loss 0.034872961961850524\n",
      "Epoch 1 batch 19999 loss 0.03455020887488499\n",
      "Epoch 1 batch 29999 loss 0.03419948093201965\n",
      "Epoch 1 batch 39999 loss 0.033818279281258586\n",
      "Epoch 1 batch 49999 loss 0.03349617852959782\n",
      "Epoch 1 batch 59999 loss 0.03337812042217702\n",
      "Epoch 1 batch 69999 loss 0.03291070661023259\n",
      "Epoch 1 batch 79999 loss 0.0327767792920582\n",
      "Epoch 1 batch 89999 loss 0.03240397796817124\n",
      "Epoch 1 batch 99999 loss 0.032267929736245426\n",
      "Epoch 1 batch 109999 loss 0.031972321514692155\n",
      "Epoch 1 batch 119999 loss 0.03171598302666098\n",
      "Epoch 1 batch 129999 loss 0.03154854500535875\n",
      "Epoch 1 batch 139999 loss 0.03136987614743412\n",
      "Epoch 1 batch 149999 loss 0.030967991959676148\n",
      "Epoch 1 batch 159999 loss 0.030882962201163173\n",
      "Epoch 1 batch 169999 loss 0.030855206209793686\n",
      "Epoch 1 batch 179999 loss 0.030483773694653063\n",
      "Epoch 1 batch 189999 loss 0.03029443908324465\n",
      "Epoch 1 batch 199999 loss 0.030088714511692524\n",
      "Epoch 1 batch 209999 loss 0.029904810043703763\n",
      "Epoch 1 batch 219999 loss 0.029686560628563164\n",
      "Epoch 1 batch 229999 loss 0.029541155470628293\n",
      "Epoch 1 batch 239999 loss 0.029435617411788552\n",
      "Epoch 1 batch 249999 loss 0.02942367615913972\n",
      "Epoch 1 batch 259999 loss 0.02917876437213272\n",
      "Epoch 1 batch 269999 loss 0.028984431418590247\n",
      "Epoch 1 batch 279999 loss 0.028837006898783147\n",
      "Epoch 1 batch 289999 loss 0.028712746116332708\n",
      "Epoch 1 batch 299999 loss 0.0285660599431023\n",
      "Epoch 1 batch 309999 loss 0.028552344402391464\n",
      "Epoch 1 batch 319999 loss 0.028207773207407444\n",
      "Epoch 1 batch 329999 loss 0.02820460285069421\n",
      "Epoch 1 batch 339999 loss 0.028001256033126264\n",
      "Epoch 1 batch 349999 loss 0.027802589628566056\n",
      "Epoch 1 batch 359999 loss 0.02782187954010442\n",
      "Epoch 1 batch 369999 loss 0.02778326815161854\n",
      "Epoch 1 batch 379999 loss 0.027552030522562564\n",
      "Epoch 1 batch 389999 loss 0.027434836419019847\n",
      "Epoch 1 batch 399999 loss 0.02743951681861654\n",
      "Epoch 1 batch 409999 loss 0.027246783821564168\n",
      "Epoch 1 batch 419999 loss 0.027137711092922838\n",
      "Epoch 1 batch 429999 loss 0.027140589090436696\n",
      "Epoch 1 batch 439999 loss 0.027125499300844968\n",
      "Epoch 1 batch 449999 loss 0.026860922121163457\n",
      "Epoch 1 batch 459999 loss 0.02677720744702965\n",
      "Epoch 1 batch 469999 loss 0.026775712159089745\n",
      "Epoch 1 batch 479999 loss 0.02664745682142675\n",
      "Epoch 1 batch 489999 loss 0.026674931258149445\n",
      "Epoch 1 batch 499999 loss 0.02644584219176322\n",
      "Epoch 1 batch 509999 loss 0.02645537208626047\n",
      "Epoch 1 batch 519999 loss 0.026315537205711007\n",
      "Epoch 1 batch 529999 loss 0.026210606059990822\n",
      "Epoch 1 batch 539999 loss 0.026128023842629047\n",
      "VALIDATION: Epoch 1 loss 0.02608012204840309\n",
      "Epoch 2 batch 9999 loss 0.026031492853537203\n",
      "Epoch 2 batch 19999 loss 0.02581363373827189\n",
      "Epoch 2 batch 29999 loss 0.025710666200891137\n",
      "Epoch 2 batch 39999 loss 0.02572051890650764\n",
      "Epoch 2 batch 49999 loss 0.025649273757915945\n",
      "Epoch 2 batch 59999 loss 0.025675707335211337\n",
      "Epoch 2 batch 69999 loss 0.025557462586369366\n",
      "Epoch 2 batch 79999 loss 0.025349340993072837\n",
      "Epoch 2 batch 89999 loss 0.02540609295228496\n",
      "Epoch 2 batch 99999 loss 0.025389387531206013\n",
      "Epoch 2 batch 109999 loss 0.025318098443001508\n",
      "Epoch 2 batch 119999 loss 0.025225073963031173\n",
      "Epoch 2 batch 129999 loss 0.025201972018368542\n",
      "Epoch 2 batch 139999 loss 0.025087607535067946\n",
      "Epoch 2 batch 149999 loss 0.025135900106281042\n",
      "Epoch 2 batch 159999 loss 0.024987254376988857\n",
      "Epoch 2 batch 169999 loss 0.024970340807270258\n",
      "Epoch 2 batch 179999 loss 0.024908515308704228\n",
      "Epoch 2 batch 189999 loss 0.024838180310744792\n",
      "Epoch 2 batch 199999 loss 0.024759150323923677\n",
      "Epoch 2 batch 209999 loss 0.024821928050834687\n",
      "Epoch 2 batch 219999 loss 0.02471599451806396\n",
      "Epoch 2 batch 229999 loss 0.02463038711035624\n",
      "Epoch 2 batch 239999 loss 0.024555601354781537\n",
      "Epoch 2 batch 249999 loss 0.024623948547709735\n",
      "Epoch 2 batch 259999 loss 0.024490229554101826\n",
      "Epoch 2 batch 269999 loss 0.02453888264708221\n",
      "Epoch 2 batch 279999 loss 0.02447368168262765\n",
      "Epoch 2 batch 289999 loss 0.02439428366338834\n",
      "Epoch 2 batch 299999 loss 0.02429673579186201\n",
      "Epoch 2 batch 309999 loss 0.024408082009200006\n",
      "Epoch 2 batch 319999 loss 0.024423250290658324\n",
      "Epoch 2 batch 329999 loss 0.02431965162185952\n",
      "Epoch 2 batch 339999 loss 0.02434400991117582\n",
      "Epoch 2 batch 349999 loss 0.024198039049655198\n",
      "Epoch 2 batch 359999 loss 0.024111631885543466\n",
      "Epoch 2 batch 369999 loss 0.024242198672890663\n",
      "Epoch 2 batch 379999 loss 0.02409169271318242\n",
      "Epoch 2 batch 389999 loss 0.024034796331822872\n",
      "Epoch 2 batch 399999 loss 0.02421927619734779\n",
      "Epoch 2 batch 409999 loss 0.024049712196271868\n",
      "Epoch 2 batch 419999 loss 0.024051704148575662\n",
      "Epoch 2 batch 429999 loss 0.024017746456246825\n",
      "Epoch 2 batch 439999 loss 0.023943090579938143\n",
      "Epoch 2 batch 449999 loss 0.02398549038628116\n",
      "Epoch 2 batch 459999 loss 0.023848798338975758\n",
      "Epoch 2 batch 469999 loss 0.023868644658010453\n",
      "Epoch 2 batch 479999 loss 0.023872354495804757\n",
      "Epoch 2 batch 489999 loss 0.023793475350830703\n",
      "Epoch 2 batch 499999 loss 0.023711496794130652\n",
      "Epoch 2 batch 509999 loss 0.023724771603569388\n",
      "Epoch 2 batch 519999 loss 0.023771184580307452\n",
      "Epoch 2 batch 529999 loss 0.02370255007511005\n",
      "Epoch 2 batch 539999 loss 0.02375233648745343\n",
      "VALIDATION: Epoch 2 loss 0.023720597318223298\n",
      "Epoch 3 batch 9999 loss 0.02347861416833475\n",
      "Epoch 3 batch 19999 loss 0.02353632728951052\n",
      "Epoch 3 batch 29999 loss 0.023383715482987462\n",
      "Epoch 3 batch 39999 loss 0.02345865105241537\n",
      "Epoch 3 batch 49999 loss 0.023461982836946844\n",
      "Epoch 3 batch 59999 loss 0.023517659193929286\n",
      "Epoch 3 batch 69999 loss 0.023437983915396036\n",
      "Epoch 3 batch 79999 loss 0.02339227570174262\n",
      "Epoch 3 batch 89999 loss 0.023299742823746056\n",
      "Epoch 3 batch 99999 loss 0.023415641072764993\n",
      "Epoch 3 batch 109999 loss 0.023409384300466626\n",
      "Epoch 3 batch 119999 loss 0.023301668140944094\n",
      "Epoch 3 batch 129999 loss 0.023356509538460524\n",
      "Epoch 3 batch 139999 loss 0.023396252494491638\n",
      "Epoch 3 batch 149999 loss 0.02322941250121221\n",
      "Epoch 3 batch 159999 loss 0.023315736564062536\n",
      "Epoch 3 batch 169999 loss 0.0232914879322052\n",
      "Epoch 3 batch 179999 loss 0.02329798401622102\n",
      "Epoch 3 batch 189999 loss 0.023217429377650842\n",
      "Epoch 3 batch 199999 loss 0.023302969720214606\n",
      "Epoch 3 batch 209999 loss 0.023218026356585323\n",
      "Epoch 3 batch 219999 loss 0.02320333114415407\n",
      "Epoch 3 batch 229999 loss 0.023131304362602533\n",
      "Epoch 3 batch 239999 loss 0.023155343690142036\n",
      "Epoch 3 batch 249999 loss 0.023107228857278825\n",
      "Epoch 3 batch 259999 loss 0.023127718733437358\n",
      "Epoch 3 batch 269999 loss 0.023100250339787454\n",
      "Epoch 3 batch 279999 loss 0.023070431450009345\n",
      "Epoch 3 batch 289999 loss 0.02307932193335146\n",
      "Epoch 3 batch 299999 loss 0.023058675728552042\n",
      "Epoch 3 batch 309999 loss 0.02301984150800854\n",
      "Epoch 3 batch 319999 loss 0.023007817277312277\n",
      "Epoch 3 batch 329999 loss 0.02296033976553008\n",
      "Epoch 3 batch 339999 loss 0.023046718104369937\n",
      "Epoch 3 batch 349999 loss 0.023054625738505273\n",
      "Epoch 3 batch 359999 loss 0.022968313438259065\n",
      "Epoch 3 batch 369999 loss 0.02293256951877847\n",
      "Epoch 3 batch 379999 loss 0.022968674293067308\n",
      "Epoch 3 batch 389999 loss 0.022977914536464958\n",
      "Epoch 3 batch 399999 loss 0.022897500551771373\n",
      "Epoch 3 batch 409999 loss 0.022982127565518023\n",
      "Epoch 3 batch 419999 loss 0.022901201468799263\n",
      "Epoch 3 batch 429999 loss 0.022968912312295288\n",
      "Epoch 3 batch 439999 loss 0.022942541157547385\n",
      "Epoch 3 batch 449999 loss 0.022958808099944144\n",
      "Epoch 3 batch 459999 loss 0.02291732811369002\n",
      "Epoch 3 batch 469999 loss 0.022826660918164998\n",
      "Epoch 3 batch 479999 loss 0.02282474490990862\n",
      "Epoch 3 batch 489999 loss 0.022805958120245485\n",
      "Epoch 3 batch 499999 loss 0.02280810242919251\n",
      "Epoch 3 batch 509999 loss 0.022780799358617515\n",
      "Epoch 3 batch 519999 loss 0.022776820688135923\n",
      "Epoch 3 batch 529999 loss 0.022802061606477946\n",
      "Epoch 3 batch 539999 loss 0.02265305558182299\n",
      "VALIDATION: Epoch 3 loss 0.02285266912838728\n",
      "Epoch 4 batch 9999 loss 0.02256205047024414\n",
      "Epoch 4 batch 19999 loss 0.022672800968028605\n",
      "Epoch 4 batch 29999 loss 0.022599895873479544\n",
      "Epoch 4 batch 39999 loss 0.02264173219176009\n",
      "Epoch 4 batch 49999 loss 0.022724905714672058\n",
      "Epoch 4 batch 59999 loss 0.022693981361202896\n",
      "Epoch 4 batch 69999 loss 0.022641024849191307\n",
      "Epoch 4 batch 79999 loss 0.02263455337220803\n",
      "Epoch 4 batch 89999 loss 0.022614809637703\n",
      "Epoch 4 batch 99999 loss 0.02259164061099291\n",
      "Epoch 4 batch 109999 loss 0.022644329576101154\n",
      "Epoch 4 batch 119999 loss 0.02259294457035139\n",
      "Epoch 4 batch 129999 loss 0.022687830905150624\n",
      "Epoch 4 batch 139999 loss 0.02262113129151985\n",
      "Epoch 4 batch 149999 loss 0.022624592561461033\n",
      "Epoch 4 batch 159999 loss 0.02257166395187378\n",
      "Epoch 4 batch 169999 loss 0.022636628085793926\n",
      "Epoch 4 batch 179999 loss 0.0224852304674685\n",
      "Epoch 4 batch 189999 loss 0.0225463055789005\n",
      "Epoch 4 batch 199999 loss 0.022526329165324568\n",
      "Epoch 4 batch 209999 loss 0.02253030830398202\n",
      "Epoch 4 batch 219999 loss 0.02257760277092457\n",
      "Epoch 4 batch 229999 loss 0.022524789400119333\n",
      "Epoch 4 batch 239999 loss 0.022580874627642335\n",
      "Epoch 4 batch 249999 loss 0.022563571236468852\n",
      "Epoch 4 batch 259999 loss 0.0224173949460499\n",
      "Epoch 4 batch 269999 loss 0.022412827449385078\n",
      "Epoch 4 batch 279999 loss 0.022451558852102607\n",
      "Epoch 4 batch 289999 loss 0.02251827480830252\n",
      "Epoch 4 batch 299999 loss 0.022506843257695437\n",
      "Epoch 4 batch 309999 loss 0.022436062453873456\n",
      "Epoch 4 batch 319999 loss 0.022510965753067286\n",
      "Epoch 4 batch 329999 loss 0.022480071092024444\n",
      "Epoch 4 batch 339999 loss 0.022486552917584775\n",
      "Epoch 4 batch 349999 loss 0.022461512896232307\n",
      "Epoch 4 batch 359999 loss 0.02245603179540485\n",
      "Epoch 4 batch 369999 loss 0.022256361311813817\n",
      "Epoch 4 batch 379999 loss 0.022392646269500254\n",
      "Epoch 4 batch 389999 loss 0.02246427598912269\n",
      "Epoch 4 batch 399999 loss 0.02248765289001167\n",
      "Epoch 4 batch 409999 loss 0.022396546929888426\n",
      "Epoch 4 batch 419999 loss 0.022405642904900016\n",
      "Epoch 4 batch 429999 loss 0.022324161300808193\n",
      "Epoch 4 batch 439999 loss 0.022384177148807794\n",
      "Epoch 4 batch 449999 loss 0.02234088634075597\n",
      "Epoch 4 batch 459999 loss 0.02235874866945669\n",
      "Epoch 4 batch 469999 loss 0.022384153285156937\n",
      "Epoch 4 batch 479999 loss 0.022424323372915386\n",
      "Epoch 4 batch 489999 loss 0.022334602545294912\n",
      "Epoch 4 batch 499999 loss 0.022376605501864107\n",
      "Epoch 4 batch 509999 loss 0.022385890301223843\n",
      "Epoch 4 batch 519999 loss 0.02232581466510892\n",
      "Epoch 4 batch 529999 loss 0.022422784444596618\n",
      "Epoch 4 batch 539999 loss 0.022340970777068286\n",
      "VALIDATION: Epoch 4 loss 0.022435650945959378\n",
      "Epoch 5 batch 9999 loss 0.022280293706525116\n",
      "Epoch 5 batch 19999 loss 0.022282935797702522\n",
      "Epoch 5 batch 29999 loss 0.022214801137614995\n",
      "Epoch 5 batch 39999 loss 0.022234898249059915\n",
      "Epoch 5 batch 49999 loss 0.022216081084590406\n",
      "Epoch 5 batch 59999 loss 0.022234096185397356\n",
      "Epoch 5 batch 69999 loss 0.02221402198905125\n",
      "Epoch 5 batch 79999 loss 0.022196345033403488\n",
      "Epoch 5 batch 89999 loss 0.022206259023863823\n",
      "Epoch 5 batch 99999 loss 0.022219654114637524\n",
      "Epoch 5 batch 109999 loss 0.022219129127729685\n",
      "Epoch 5 batch 119999 loss 0.0221845277373679\n",
      "Epoch 5 batch 129999 loss 0.022192219412419945\n",
      "Epoch 5 batch 139999 loss 0.022258807188645004\n",
      "Epoch 5 batch 149999 loss 0.022181752676237374\n",
      "Epoch 5 batch 159999 loss 0.022241694531403483\n",
      "Epoch 5 batch 169999 loss 0.02226474102186039\n",
      "Epoch 5 batch 179999 loss 0.022130816233251242\n",
      "Epoch 5 batch 189999 loss 0.02218015182064846\n",
      "Epoch 5 batch 199999 loss 0.022149458065442742\n",
      "Epoch 5 batch 209999 loss 0.02225713605247438\n",
      "Epoch 5 batch 219999 loss 0.022239352229237556\n",
      "Epoch 5 batch 229999 loss 0.02225057625817135\n",
      "Epoch 5 batch 239999 loss 0.022155498259235174\n",
      "Epoch 5 batch 249999 loss 0.022194308930868285\n",
      "Epoch 5 batch 259999 loss 0.022183882038947195\n",
      "Epoch 5 batch 269999 loss 0.0222458847229369\n",
      "Epoch 5 batch 279999 loss 0.022185323763825\n",
      "Epoch 5 batch 289999 loss 0.02218316143695265\n",
      "Epoch 5 batch 299999 loss 0.02217815419761464\n",
      "Epoch 5 batch 309999 loss 0.022121418568771332\n",
      "Epoch 5 batch 319999 loss 0.02222133831558749\n",
      "Epoch 5 batch 329999 loss 0.02218263974217698\n",
      "Epoch 5 batch 339999 loss 0.022220540860760956\n",
      "Epoch 5 batch 349999 loss 0.0221697916877456\n",
      "Epoch 5 batch 359999 loss 0.02222721191290766\n",
      "Epoch 5 batch 369999 loss 0.022149802681058647\n",
      "Epoch 5 batch 379999 loss 0.022128955820202828\n",
      "Epoch 5 batch 389999 loss 0.022252837391290813\n",
      "Epoch 5 batch 399999 loss 0.02212664880156517\n",
      "Epoch 5 batch 409999 loss 0.02210256208255887\n",
      "Epoch 5 batch 419999 loss 0.02213522544018924\n",
      "Epoch 5 batch 429999 loss 0.02219953727275133\n",
      "Epoch 5 batch 439999 loss 0.022107232683710756\n",
      "Epoch 5 batch 449999 loss 0.02213981327600777\n",
      "Epoch 5 batch 459999 loss 0.022115850166138262\n",
      "Epoch 5 batch 469999 loss 0.02221941013885662\n",
      "Epoch 5 batch 479999 loss 0.022163306776247917\n",
      "Epoch 5 batch 489999 loss 0.02213173914235085\n",
      "Epoch 5 batch 499999 loss 0.022074755957350135\n",
      "Epoch 5 batch 509999 loss 0.02215764335813001\n",
      "Epoch 5 batch 519999 loss 0.02208848487250507\n",
      "Epoch 5 batch 529999 loss 0.02205553416516632\n",
      "Epoch 5 batch 539999 loss 0.021999790615029633\n",
      "VALIDATION: Epoch 5 loss 0.0222049846313054\n",
      "Epoch 6 batch 9999 loss 0.021990067570935936\n",
      "Epoch 6 batch 19999 loss 0.021971139006875456\n",
      "Epoch 6 batch 29999 loss 0.02198117155423388\n",
      "Epoch 6 batch 39999 loss 0.022038893702719362\n",
      "Epoch 6 batch 49999 loss 0.02209280407456681\n",
      "Epoch 6 batch 59999 loss 0.022020751683274283\n",
      "Epoch 6 batch 69999 loss 0.02205725050969049\n",
      "Epoch 6 batch 79999 loss 0.02201541282213293\n",
      "Epoch 6 batch 89999 loss 0.021908083470351995\n",
      "Epoch 6 batch 99999 loss 0.022027546913642435\n",
      "Epoch 6 batch 109999 loss 0.022067083338368685\n",
      "Epoch 6 batch 119999 loss 0.022090170872583987\n",
      "Epoch 6 batch 129999 loss 0.022013734807260335\n",
      "Epoch 6 batch 139999 loss 0.02209860145319253\n",
      "Epoch 6 batch 149999 loss 0.022011360652837902\n",
      "Epoch 6 batch 159999 loss 0.021979915520735086\n",
      "Epoch 6 batch 169999 loss 0.022064831202011557\n",
      "Epoch 6 batch 179999 loss 0.02204619419267401\n",
      "Epoch 6 batch 189999 loss 0.022007900370983408\n",
      "Epoch 6 batch 199999 loss 0.021985052022058516\n",
      "Epoch 6 batch 209999 loss 0.021992329161707312\n",
      "Epoch 6 batch 219999 loss 0.022028679891303183\n",
      "Epoch 6 batch 229999 loss 0.021992052460089326\n",
      "Epoch 6 batch 239999 loss 0.021987781548779457\n",
      "Epoch 6 batch 249999 loss 0.02194161886209622\n",
      "Epoch 6 batch 259999 loss 0.021913870474603026\n",
      "Epoch 6 batch 269999 loss 0.02195722242044285\n",
      "Epoch 6 batch 279999 loss 0.02207271727528423\n",
      "Epoch 6 batch 289999 loss 0.022010680828709155\n",
      "Epoch 6 batch 299999 loss 0.022047408665996044\n",
      "Epoch 6 batch 309999 loss 0.02201068522105925\n",
      "Epoch 6 batch 319999 loss 0.021970990370679646\n",
      "Epoch 6 batch 329999 loss 0.02206269061425701\n",
      "Epoch 6 batch 339999 loss 0.021997521112114192\n",
      "Epoch 6 batch 349999 loss 0.021959541201591493\n",
      "Epoch 6 batch 359999 loss 0.021981286894436926\n",
      "Epoch 6 batch 369999 loss 0.021984184927213938\n",
      "Epoch 6 batch 379999 loss 0.0219486723376438\n",
      "Epoch 6 batch 389999 loss 0.02202876229090616\n",
      "Epoch 6 batch 399999 loss 0.022031816220935436\n",
      "Epoch 6 batch 409999 loss 0.021913427423592657\n",
      "Epoch 6 batch 419999 loss 0.021958549814298747\n",
      "Epoch 6 batch 429999 loss 0.02198375660981983\n",
      "Epoch 6 batch 439999 loss 0.022029050167743116\n",
      "Epoch 6 batch 449999 loss 0.022024544956022872\n",
      "Epoch 6 batch 459999 loss 0.02203045327132568\n",
      "Epoch 6 batch 469999 loss 0.021987117509357633\n",
      "Epoch 6 batch 479999 loss 0.02198969713989645\n",
      "Epoch 6 batch 489999 loss 0.021915063546597956\n",
      "Epoch 6 batch 499999 loss 0.021960212662257256\n",
      "Epoch 6 batch 509999 loss 0.022001557100750507\n",
      "Epoch 6 batch 519999 loss 0.021978309596981854\n",
      "Epoch 6 batch 529999 loss 0.022037396025750788\n",
      "Epoch 6 batch 539999 loss 0.021937472093943505\n",
      "VALIDATION: Epoch 6 loss 0.022075608190286793\n",
      "Epoch 7 batch 9999 loss 0.021884990786481647\n",
      "Epoch 7 batch 19999 loss 0.021928539025597275\n",
      "Epoch 7 batch 29999 loss 0.02172685731947422\n",
      "Epoch 7 batch 39999 loss 0.02181834488539025\n",
      "Epoch 7 batch 49999 loss 0.0218253578142263\n",
      "Epoch 7 batch 59999 loss 0.021924398491624742\n",
      "Epoch 7 batch 69999 loss 0.02182999547654763\n",
      "Epoch 7 batch 79999 loss 0.021958315198402852\n",
      "Epoch 7 batch 89999 loss 0.021832821015967056\n",
      "Epoch 7 batch 99999 loss 0.021933541379123928\n",
      "Epoch 7 batch 109999 loss 0.02190920372353867\n",
      "Epoch 7 batch 119999 loss 0.02186373721221462\n",
      "Epoch 7 batch 129999 loss 0.02185555267399177\n",
      "Epoch 7 batch 139999 loss 0.0219831875262782\n",
      "Epoch 7 batch 149999 loss 0.021939494975283743\n",
      "Epoch 7 batch 159999 loss 0.021905251178564504\n",
      "Epoch 7 batch 169999 loss 0.021858609597943723\n",
      "Epoch 7 batch 179999 loss 0.02176363861691207\n",
      "Epoch 7 batch 189999 loss 0.021882683152519167\n",
      "Epoch 7 batch 199999 loss 0.021836161813605575\n",
      "Epoch 7 batch 209999 loss 0.021834716448094697\n",
      "Epoch 7 batch 219999 loss 0.021892014535982163\n",
      "Epoch 7 batch 229999 loss 0.021891776381619276\n",
      "Epoch 7 batch 239999 loss 0.022028881369531156\n",
      "Epoch 7 batch 249999 loss 0.02187295874496922\n",
      "Epoch 7 batch 259999 loss 0.02195391073767096\n",
      "Epoch 7 batch 269999 loss 0.021937743892986326\n",
      "Epoch 7 batch 279999 loss 0.021847611856088044\n",
      "Epoch 7 batch 289999 loss 0.021954882078105583\n",
      "Epoch 7 batch 299999 loss 0.021954906490817667\n",
      "Epoch 7 batch 309999 loss 0.02187556467782706\n",
      "Epoch 7 batch 319999 loss 0.021898719155136494\n",
      "Epoch 7 batch 329999 loss 0.021990290603786707\n",
      "Epoch 7 batch 339999 loss 0.021990897945594043\n",
      "Epoch 7 batch 349999 loss 0.02186903737699613\n",
      "Epoch 7 batch 359999 loss 0.02191289538247511\n",
      "Epoch 7 batch 369999 loss 0.02193017493579537\n",
      "Epoch 7 batch 379999 loss 0.021883583831042053\n",
      "Epoch 7 batch 389999 loss 0.02192935526096262\n",
      "Epoch 7 batch 399999 loss 0.022016197630390526\n",
      "Epoch 7 batch 409999 loss 0.02193459033817053\n",
      "Epoch 7 batch 419999 loss 0.021960421663429586\n",
      "Epoch 7 batch 429999 loss 0.021920707055926324\n",
      "Epoch 7 batch 439999 loss 0.021871067174430936\n",
      "Epoch 7 batch 449999 loss 0.02187808289118111\n",
      "Epoch 7 batch 459999 loss 0.021933215846214445\n",
      "Epoch 7 batch 469999 loss 0.021942093191109596\n",
      "Epoch 7 batch 479999 loss 0.021946299841906877\n",
      "Epoch 7 batch 489999 loss 0.021881305395625533\n",
      "Epoch 7 batch 499999 loss 0.021845227974373847\n",
      "Epoch 7 batch 509999 loss 0.0219008698284626\n",
      "Epoch 7 batch 519999 loss 0.02188206077264622\n",
      "Epoch 7 batch 529999 loss 0.021979344277922065\n",
      "Epoch 7 batch 539999 loss 0.02191755884764716\n",
      "VALIDATION: Epoch 7 loss 0.02199161775658752\n",
      "Epoch 8 batch 9999 loss 0.021770541939139368\n",
      "Epoch 8 batch 19999 loss 0.021837374681793153\n",
      "Epoch 8 batch 29999 loss 0.021713902780786158\n",
      "Epoch 8 batch 39999 loss 0.021775843447912484\n",
      "Epoch 8 batch 49999 loss 0.02177252987921238\n",
      "Epoch 8 batch 59999 loss 0.021828953223954888\n",
      "Epoch 8 batch 69999 loss 0.021835793454758824\n",
      "Epoch 8 batch 79999 loss 0.0219026706579607\n",
      "Epoch 8 batch 89999 loss 0.021893675400782377\n",
      "Epoch 8 batch 99999 loss 0.021817259818688036\n",
      "Epoch 8 batch 109999 loss 0.021829495670367034\n",
      "Epoch 8 batch 119999 loss 0.021775254043284804\n",
      "Epoch 8 batch 129999 loss 0.02182388185095042\n",
      "Epoch 8 batch 139999 loss 0.021869962923321874\n",
      "Epoch 8 batch 149999 loss 0.021879801985388622\n",
      "Epoch 8 batch 159999 loss 0.02176809709495865\n",
      "Epoch 8 batch 169999 loss 0.021846533928858117\n",
      "Epoch 8 batch 179999 loss 0.021922576462943107\n",
      "Epoch 8 batch 189999 loss 0.021777290122862905\n",
      "Epoch 8 batch 199999 loss 0.021876981550920754\n",
      "Epoch 8 batch 209999 loss 0.02186180490134284\n",
      "Epoch 8 batch 219999 loss 0.021916455463599414\n",
      "Epoch 8 batch 229999 loss 0.02191068709231913\n",
      "Epoch 8 batch 239999 loss 0.02183336367085576\n",
      "Epoch 8 batch 249999 loss 0.02184870392093435\n",
      "Epoch 8 batch 259999 loss 0.02188030529972166\n",
      "Epoch 8 batch 269999 loss 0.0218622715100646\n",
      "Epoch 8 batch 279999 loss 0.021829575879126786\n",
      "Epoch 8 batch 289999 loss 0.021839673419250177\n",
      "Epoch 8 batch 299999 loss 0.021785628105420618\n",
      "Epoch 8 batch 309999 loss 0.021886511083738878\n",
      "Epoch 8 batch 319999 loss 0.021845230726571754\n",
      "Epoch 8 batch 329999 loss 0.02186490881405771\n",
      "Epoch 8 batch 339999 loss 0.021900972534902394\n",
      "Epoch 8 batch 349999 loss 0.021863704552594572\n",
      "Epoch 8 batch 359999 loss 0.02192430489519611\n",
      "Epoch 8 batch 369999 loss 0.02181920744422823\n",
      "Epoch 8 batch 379999 loss 0.02178419126998633\n",
      "Epoch 8 batch 389999 loss 0.021825034385640173\n",
      "Epoch 8 batch 399999 loss 0.021862643892038614\n",
      "Epoch 8 batch 409999 loss 0.021869291434343904\n",
      "Epoch 8 batch 419999 loss 0.021907024493254722\n",
      "Epoch 8 batch 429999 loss 0.021832458259351553\n",
      "Epoch 8 batch 439999 loss 0.02187274139551446\n",
      "Epoch 8 batch 449999 loss 0.02193887217408046\n",
      "Epoch 8 batch 459999 loss 0.02183305833013728\n",
      "Epoch 8 batch 469999 loss 0.021848812805116177\n",
      "Epoch 8 batch 479999 loss 0.021880331606976687\n",
      "Epoch 8 batch 489999 loss 0.021863779542129487\n",
      "Epoch 8 batch 499999 loss 0.02186210555406287\n",
      "Epoch 8 batch 509999 loss 0.0218892308072187\n",
      "Epoch 8 batch 519999 loss 0.021899463484995066\n",
      "Epoch 8 batch 529999 loss 0.021964818591903894\n",
      "Epoch 8 batch 539999 loss 0.021909293741174042\n",
      "VALIDATION: Epoch 8 loss 0.02195392198780026\n",
      "Epoch 9 batch 9999 loss 0.02172190506774932\n",
      "Epoch 9 batch 19999 loss 0.021752355618355795\n",
      "Epoch 9 batch 29999 loss 0.02181933346921578\n",
      "Epoch 9 batch 39999 loss 0.021702747047273444\n",
      "Epoch 9 batch 49999 loss 0.021758893190138042\n",
      "Epoch 9 batch 59999 loss 0.02185644582435489\n",
      "Epoch 9 batch 69999 loss 0.02175552492281422\n",
      "Epoch 9 batch 79999 loss 0.02184270266685635\n",
      "Epoch 9 batch 89999 loss 0.021871360718645157\n",
      "Epoch 9 batch 99999 loss 0.021766896525025368\n",
      "Epoch 9 batch 109999 loss 0.02186623117546551\n",
      "Epoch 9 batch 119999 loss 0.021880395061615855\n",
      "Epoch 9 batch 129999 loss 0.021819662901153787\n",
      "Epoch 9 batch 139999 loss 0.021880068846046926\n",
      "Epoch 9 batch 149999 loss 0.021798174471221864\n",
      "Epoch 9 batch 159999 loss 0.021783191950805487\n",
      "Epoch 9 batch 169999 loss 0.021838546989671886\n",
      "Epoch 9 batch 179999 loss 0.021862310258112846\n",
      "Epoch 9 batch 189999 loss 0.02189898198004812\n",
      "Epoch 9 batch 199999 loss 0.021902709604799748\n",
      "Epoch 9 batch 209999 loss 0.021878246101364494\n",
      "Epoch 9 batch 219999 loss 0.021890860801469535\n",
      "Epoch 9 batch 229999 loss 0.021923076966265217\n",
      "Epoch 9 batch 239999 loss 0.021792341942153872\n",
      "Epoch 9 batch 249999 loss 0.021813622674159705\n",
      "Epoch 9 batch 259999 loss 0.021833712724037467\n",
      "Epoch 9 batch 269999 loss 0.02180038540456444\n",
      "Epoch 9 batch 279999 loss 0.021881578163756056\n",
      "Epoch 9 batch 289999 loss 0.02180081513626501\n",
      "Epoch 9 batch 299999 loss 0.02187697617225349\n",
      "Epoch 9 batch 309999 loss 0.02184510138463229\n",
      "Epoch 9 batch 319999 loss 0.0218619795053266\n",
      "Epoch 9 batch 329999 loss 0.02177459645131603\n",
      "Epoch 9 batch 339999 loss 0.021923575621331112\n",
      "Epoch 9 batch 349999 loss 0.021830932805128395\n",
      "Epoch 9 batch 359999 loss 0.021851354116713628\n",
      "Epoch 9 batch 369999 loss 0.021833818867569788\n",
      "Epoch 9 batch 379999 loss 0.02183347498551011\n",
      "Epoch 9 batch 389999 loss 0.02195436640251428\n",
      "Epoch 9 batch 399999 loss 0.021801303652394563\n",
      "Epoch 9 batch 409999 loss 0.02184523475281894\n",
      "Epoch 9 batch 419999 loss 0.021861429024115205\n",
      "Epoch 9 batch 429999 loss 0.021846918970532717\n",
      "Epoch 9 batch 439999 loss 0.02183438331745565\n",
      "Epoch 9 batch 449999 loss 0.021860153392422945\n",
      "Epoch 9 batch 459999 loss 0.0218113971860148\n",
      "Epoch 9 batch 469999 loss 0.021923116633296012\n",
      "Epoch 9 batch 479999 loss 0.021905000334139915\n",
      "Epoch 9 batch 489999 loss 0.021866979156248273\n",
      "Epoch 9 batch 499999 loss 0.02184956431929022\n",
      "Epoch 9 batch 509999 loss 0.021813305792585017\n",
      "Epoch 9 batch 519999 loss 0.02178615237660706\n",
      "Epoch 9 batch 529999 loss 0.021848601565882565\n",
      "Epoch 9 batch 539999 loss 0.02179636063668877\n",
      "VALIDATION: Epoch 9 loss 0.021951071628116617\n",
      "Epoch 10 batch 9999 loss 0.021659705932158976\n",
      "Epoch 10 batch 19999 loss 0.02182515689013526\n",
      "Epoch 10 batch 29999 loss 0.02175856022974476\n",
      "Epoch 10 batch 39999 loss 0.021810478664562104\n",
      "Epoch 10 batch 49999 loss 0.02184661869937554\n",
      "Epoch 10 batch 59999 loss 0.02187643235446885\n",
      "Epoch 10 batch 69999 loss 0.02173110305829905\n",
      "Epoch 10 batch 79999 loss 0.021753325927909465\n",
      "Epoch 10 batch 89999 loss 0.02180617766007781\n",
      "Epoch 10 batch 99999 loss 0.02184878179160878\n",
      "Epoch 10 batch 109999 loss 0.02176102353394963\n",
      "Epoch 10 batch 119999 loss 0.021860103739751502\n",
      "Epoch 10 batch 129999 loss 0.021847064700722695\n",
      "Epoch 10 batch 139999 loss 0.021833025368396194\n",
      "Epoch 10 batch 149999 loss 0.021870149828959257\n",
      "Epoch 10 batch 159999 loss 0.021824928003177046\n",
      "Epoch 10 batch 169999 loss 0.02189101707553491\n",
      "Epoch 10 batch 179999 loss 0.021794023358821867\n",
      "Epoch 10 batch 189999 loss 0.021817839657049625\n",
      "Epoch 10 batch 199999 loss 0.021847163575794547\n",
      "Epoch 10 batch 209999 loss 0.021935505779786035\n",
      "Epoch 10 batch 219999 loss 0.021853852361533792\n",
      "Epoch 10 batch 229999 loss 0.021793485662713646\n",
      "Epoch 10 batch 239999 loss 0.02194735533380881\n",
      "Epoch 10 batch 249999 loss 0.021839411143306644\n",
      "Epoch 10 batch 259999 loss 0.021807367298845203\n",
      "Epoch 10 batch 269999 loss 0.021770094253867864\n",
      "Epoch 10 batch 279999 loss 0.021819859008584172\n",
      "Epoch 10 batch 289999 loss 0.021876111205574124\n",
      "Epoch 10 batch 299999 loss 0.021803994346130638\n",
      "Epoch 10 batch 309999 loss 0.02179579537594691\n",
      "Epoch 10 batch 319999 loss 0.021777240760345012\n",
      "Epoch 10 batch 329999 loss 0.021882492235675455\n",
      "Epoch 10 batch 339999 loss 0.021815057034883647\n",
      "Epoch 10 batch 349999 loss 0.02187742685303092\n",
      "Epoch 10 batch 359999 loss 0.021916290326882155\n",
      "Epoch 10 batch 369999 loss 0.021835611945949494\n",
      "Epoch 10 batch 379999 loss 0.021918208918068557\n",
      "Epoch 10 batch 389999 loss 0.02191244700020179\n",
      "Epoch 10 batch 399999 loss 0.021847791201341896\n",
      "Epoch 10 batch 409999 loss 0.021875409787148237\n",
      "Epoch 10 batch 419999 loss 0.021785098089557142\n",
      "Epoch 10 batch 429999 loss 0.02191715961843729\n",
      "Epoch 10 batch 439999 loss 0.021846410649036987\n",
      "Epoch 10 batch 449999 loss 0.021890081164147705\n",
      "Epoch 10 batch 459999 loss 0.021912165414169432\n",
      "Epoch 10 batch 469999 loss 0.021780747569259257\n",
      "Epoch 10 batch 479999 loss 0.021910303224110975\n",
      "Epoch 10 batch 489999 loss 0.021968535221554338\n",
      "Epoch 10 batch 499999 loss 0.02186924753980711\n",
      "Epoch 10 batch 509999 loss 0.021819807608611883\n",
      "Epoch 10 batch 519999 loss 0.02197698015123606\n",
      "Epoch 10 batch 529999 loss 0.021891010026913137\n",
      "Epoch 10 batch 539999 loss 0.02187241507517174\n",
      "VALIDATION: Epoch 10 loss 0.0219543033717007\n",
      "Epoch 11 batch 9999 loss 0.02169433326125145\n",
      "Epoch 11 batch 19999 loss 0.021617567918170242\n",
      "Epoch 11 batch 29999 loss 0.021751138690207155\n",
      "Epoch 11 batch 39999 loss 0.021759655593615024\n",
      "Epoch 11 batch 49999 loss 0.02180337318116799\n",
      "Epoch 11 batch 59999 loss 0.021901675698906182\n",
      "Epoch 11 batch 69999 loss 0.021846367650385945\n",
      "Epoch 11 batch 79999 loss 0.0218026995963417\n",
      "Epoch 11 batch 89999 loss 0.02172037734454498\n",
      "Epoch 11 batch 99999 loss 0.021783844302315263\n",
      "Epoch 11 batch 109999 loss 0.021803969723265617\n",
      "Epoch 11 batch 119999 loss 0.02173704862333834\n",
      "Epoch 11 batch 129999 loss 0.021850947045907378\n",
      "Epoch 11 batch 139999 loss 0.021886920726764948\n",
      "Epoch 11 batch 149999 loss 0.021866032182797788\n",
      "Epoch 11 batch 159999 loss 0.02184221084024757\n",
      "Epoch 11 batch 169999 loss 0.0219255692390725\n",
      "Epoch 11 batch 179999 loss 0.021944504457898438\n",
      "Epoch 11 batch 189999 loss 0.02187884891247377\n",
      "Epoch 11 batch 199999 loss 0.021809157246071846\n",
      "Epoch 11 batch 209999 loss 0.02179063235940412\n",
      "Epoch 11 batch 219999 loss 0.021862514864560218\n",
      "Epoch 11 batch 229999 loss 0.021913881044369192\n",
      "Epoch 11 batch 239999 loss 0.02193524997672066\n",
      "Epoch 11 batch 249999 loss 0.02191575903026387\n",
      "Epoch 11 batch 259999 loss 0.0218665040994063\n",
      "Epoch 11 batch 269999 loss 0.021942261433508248\n",
      "Epoch 11 batch 279999 loss 0.021795003708731383\n",
      "Epoch 11 batch 289999 loss 0.021842373061180115\n",
      "Epoch 11 batch 299999 loss 0.02181360743837431\n",
      "Epoch 11 batch 309999 loss 0.021795978246908636\n",
      "Epoch 11 batch 319999 loss 0.02189374023610726\n",
      "Epoch 11 batch 329999 loss 0.021855799056682737\n",
      "Epoch 11 batch 339999 loss 0.02181021780362353\n",
      "Epoch 11 batch 349999 loss 0.02181846986701712\n",
      "Epoch 11 batch 359999 loss 0.021943381897360087\n",
      "Epoch 11 batch 369999 loss 0.021949269965756683\n",
      "Epoch 11 batch 379999 loss 0.02183227629000321\n",
      "Epoch 11 batch 389999 loss 0.02191131849428639\n",
      "Epoch 11 batch 399999 loss 0.02187069266270846\n",
      "Epoch 11 batch 409999 loss 0.021898196285031736\n",
      "Epoch 11 batch 419999 loss 0.021915060838125645\n",
      "Epoch 11 batch 429999 loss 0.021871820279629903\n",
      "Epoch 11 batch 439999 loss 0.021911914616450666\n",
      "Epoch 11 batch 449999 loss 0.021919825316779315\n",
      "Epoch 11 batch 459999 loss 0.021883123339014127\n",
      "Epoch 11 batch 469999 loss 0.021981238132482393\n",
      "Epoch 11 batch 479999 loss 0.021942483552359046\n",
      "Epoch 11 batch 489999 loss 0.021873480810504407\n",
      "Epoch 11 batch 499999 loss 0.021935410304088146\n",
      "Epoch 11 batch 509999 loss 0.021974217534251512\n",
      "Epoch 11 batch 519999 loss 0.021971985141653567\n",
      "Epoch 11 batch 529999 loss 0.021993988504074514\n",
      "Epoch 11 batch 539999 loss 0.021817358672898263\n",
      "VALIDATION: Epoch 11 loss 0.02196098476987604\n",
      "CPU times: user 3d 18h 43min 54s, sys: 19min 6s, total: 3d 19h 3min 1s\n",
      "Wall time: 14h 17min 2s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.02196098476987604"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "id": "d5985e2a",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "id": "20cc600c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T11:51:04.736645Z",
     "start_time": "2024-12-13T11:51:03.635847Z"
    }
   },
   "source": [
    "model.eval()\n",
    "local_model_path = local_model_path_prefix+\".pth\"\n",
    "torch.save(model, local_model_path)\n",
    "upload_file_to_s3(local_model_path, remote_model_path)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Hyperparameter search",
   "id": "18ef2842ab8683ce"
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "def hyperopt_objective_function(train_data, \n",
    "                                val_data, \n",
    "                                bi_encoder_vocab_size,\n",
    "                                max_tokens,\n",
    "                                pad_token,\n",
    "                                verbose=True,\n",
    "                               ):\n",
    "    \n",
    "    def objective(config):\n",
    "        learning_rate = config['learning_rate']\n",
    "        batch_size = config['batch_size']\n",
    "        embedding_dim = config['embedding_dim']\n",
    "        num_epochs = config['num_epochs']\n",
    "        \n",
    "        if verbose:\n",
    "            print('train', config)\n",
    "        \n",
    "        # Create an instance of the bi-encoder model\n",
    "        model = BiEncoder(embedding_dim, bi_encoder_vocab_size, max_tokens, pad_token)\n",
    "        # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        # model.to(device)\n",
    "\n",
    "        # Define the optimizer\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=learning_rate, amsgrad=use_amsgrad)\n",
    "\n",
    "        # Create data loader\n",
    "        train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True) \n",
    "\n",
    "        val_loss = train(model, train_loader, val_loader, loss_fn, optimizer, num_epochs, verbose=False)\n",
    "        if verbose:\n",
    "            print('val_loss', val_loss)\n",
    "        \n",
    "        return {\n",
    "            'status': STATUS_OK,\n",
    "            'loss': val_loss,\n",
    "            'config': config,            \n",
    "        }\n",
    "\n",
    "    return objective\n"
   ],
   "id": "14410d54c9542c7c"
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "# HyperOpt search space\n",
    "search_space = {\n",
    "    \"learning_rate\": hp.loguniform('learning_rate', math.log(1e-4), math.log(1e-2)),\n",
    "    \"batch_size\": hp.choice('batch_size', [8, 16, 32, 64]),\n",
    "    \"embedding_dim\": hp.choice('embedding_dim', [8, 16, 32, 64]),\n",
    "    \"num_epochs\": hp.choice('num_epochs', [5, 10, 20, 40]),\n",
    "}\n",
    "objective = hyperopt_objective_function(train_data=train_data,\n",
    "                                        val_data=val_data,\n",
    "                                        bi_encoder_vocab_size=len(tokenizer_vocab),\n",
    "                                        max_tokens=max_tokens,\n",
    "                                        pad_token=tokenizer_vocab['[PAD]'],\n",
    "                                        verbose=True,\n",
    "                                        )\n",
    "trials = Trials()\n",
    "\n",
    "# minimize the objective over the space\n",
    "best = fmin(objective,\n",
    "            search_space,\n",
    "            algo=tpe.suggest,\n",
    "            trials=trials,\n",
    "            max_evals=50)"
   ],
   "id": "4280438b8839cc54"
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "print(\"best\", best)\n",
    "print(\"results\", trials.results)"
   ],
   "id": "a12e80101a211c0d"
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "batch_size = best_result.config['batch_size']\n",
    "learning_rate = best_result.config['learning_rate']\n",
    "embedding_dim = best_result.config['embedding_dim']\n",
    "num_epochs = best_result.config['num_epochs']"
   ],
   "id": "70a8be2a4cc029b5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
