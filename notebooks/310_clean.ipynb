{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-10-21T18:55:32.742179Z",
     "start_time": "2024-10-21T18:55:32.712359Z"
    }
   },
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Clean raw names and split into separate given and surname datasets"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-10-21T19:36:20.005177Z",
     "start_time": "2024-10-21T19:36:19.969762Z"
    }
   },
   "source": [
    "from os.path import join\n",
    "\n",
    "import csv\n",
    "from mpire import WorkerPool\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from nama.data.filesystem import glob, download_file_from_s3, save_file\n",
    "from nama.data.normalize import normalize"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-10-21T18:55:38.921741Z",
     "start_time": "2024-10-21T18:55:38.900255Z"
    }
   },
   "source": [
    "in_path = \"s3://fs-nama-data/2024/familysearch-names/raw/tree-hr/\"\n",
    "given_out_path = \"s3://fs-nama-data/2024/familysearch-names/interim/tree-hr-given/\"\n",
    "surname_out_path = \"s3://fs-nama-data/2024/familysearch-names/interim/tree-hr-surname/\""
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-10-21T18:55:38.953563Z",
     "start_time": "2024-10-21T18:55:38.923152Z"
    }
   },
   "source": [
    "def normalize_given_and_join(name):\n",
    "    return (\" \".join(normalize(name, is_surname=False, dont_return_empty=False))).strip()\n",
    "\n",
    "def normalize_surname_and_join(name):\n",
    "    return (\" \".join(normalize(name, is_surname=True, dont_return_empty=False))).strip()"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-10-21T19:36:21.930242Z",
     "start_time": "2024-10-21T19:36:21.877034Z"
    }
   },
   "source": [
    "def process_file(shared, filename):\n",
    "    given_out_path, surname_out_path = shared\n",
    "    basename = Path(filename).stem\n",
    "\n",
    "    filename = download_file_from_s3(filename) if filename.startswith(\"s3://\") else filename\n",
    "    \n",
    "    df = pd.read_csv(\n",
    "        filename,\n",
    "        sep=\"|\",\n",
    "        compression=\"gzip\",\n",
    "        names=[\"name\", \"alt_name\"],\n",
    "        dtype={\"name\": str, \"alt_name\": str},\n",
    "        na_filter=False,\n",
    "        encoding=\"utf-8\",\n",
    "        quoting=csv.QUOTE_NONE,\n",
    "    )\n",
    "\n",
    "    # create separate given and surname dataframes\n",
    "    given_df = df[[\"name\", \"alt_name\"]].copy()\n",
    "    surname_df = df[[\"name\", \"alt_name\"]].copy()\n",
    "    del df\n",
    "\n",
    "    # split names into given and surname\n",
    "    given_df[\"name\"] = given_df[\"name\"].str.replace(r\"\\^.*$\", \"\", regex=True)\n",
    "    given_df[\"alt_name\"] = given_df[\"alt_name\"].str.replace(r\"\\^.*$\", \"\", regex=True)\n",
    "    surname_df[\"name\"] = surname_df[\"name\"].str.replace(r\"^.*\\^\", \"\", regex=True)\n",
    "    surname_df[\"alt_name\"] = surname_df[\"alt_name\"].str.replace(r\"^.*\\^\", \"\", regex=True)\n",
    "\n",
    "    # filter out non-latin names\n",
    "    given_df = given_df[\n",
    "        given_df[\"name\"].str.endswith(\"~Latn\")\n",
    "        & given_df[\"alt_name\"].str.endswith(\"~Latn\")\n",
    "        ]\n",
    "    surname_df = surname_df[\n",
    "        surname_df[\"name\"].str.endswith(\"~Latn\")\n",
    "        & surname_df[\"alt_name\"].str.endswith(\"~Latn\")\n",
    "        ]\n",
    "\n",
    "    # remove ~Latn suffix\n",
    "    given_df[\"name\"] = given_df[\"name\"].str.replace(\n",
    "        \"~Latn$\", \"\", regex=True\n",
    "    )\n",
    "    given_df[\"alt_name\"] = given_df[\"alt_name\"].str.replace(\n",
    "        \"~Latn$\", \"\", regex=True\n",
    "    )\n",
    "    surname_df[\"name\"] = surname_df[\"name\"].str.replace(\n",
    "        \"~Latn$\", \"\", regex=True\n",
    "    )\n",
    "    surname_df[\"alt_name\"] = surname_df[\"alt_name\"].str.replace(\n",
    "        \"~Latn$\", \"\", regex=True\n",
    "    )\n",
    "\n",
    "    # normalize names and join the pieces back into a single space-separated string\n",
    "    given_df[\"name\"] = given_df[\"name\"].map(normalize_given_and_join)\n",
    "    given_df[\"alt_name\"] = given_df[\"alt_name\"].map(normalize_given_and_join)\n",
    "    surname_df[\"name\"] = surname_df[\"name\"].map(normalize_surname_and_join)\n",
    "    surname_df[\"alt_name\"] = surname_df[\"alt_name\"].map(normalize_surname_and_join)\n",
    "\n",
    "    # remove empty names\n",
    "    given_df = given_df[(given_df[\"name\"] != \"\") & (given_df[\"alt_name\"] != \"\")]\n",
    "    surname_df = surname_df[(surname_df[\"name\"] != \"\") & (surname_df[\"alt_name\"] != \"\")]\n",
    "    \n",
    "    # remove exact matches\n",
    "    given_df = given_df[given_df[\"name\"] != given_df[\"alt_name\"]]\n",
    "    surname_df = surname_df[surname_df[\"name\"] != surname_df[\"alt_name\"]]\n",
    "\n",
    "    # write files\n",
    "    save_file(join(given_out_path, basename) + \".parquet\", \n",
    "              lambda local_out_path : given_df.to_parquet(local_out_path, engine=\"pyarrow\", compression=\"snappy\"))\n",
    "    save_file(join(surname_out_path, basename) + \".parquet\",\n",
    "              lambda local_out_path : surname_df.to_parquet(local_out_path, engine=\"pyarrow\", compression=\"snappy\"))        "
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-10-22T11:32:28.223236Z",
     "start_time": "2024-10-21T19:37:57.290853Z"
    }
   },
   "source": [
    "# process files\n",
    "filenames = glob(join(in_path,\"*.gz\"))\n",
    "print(len(filenames))\n",
    "with WorkerPool(shared_objects=(given_out_path, surname_out_path)) as pool:\n",
    "    pool.map(process_file, filenames, progress_bar=True)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12000/12000 [15:54:24<00:00, 28.94s/it]  \n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T19:37:53.097190Z",
     "start_time": "2024-10-21T19:37:53.060159Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# filename = \"s3://fs-nama-data/2024/familysearch-names/raw/tree-hr/part-00063.gz\"\n",
    "# process_file((given_out_path, surname_out_path), filename)"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7a8115de58066eb2ca8cfe29f0da77ee43c62745e34e8f6d96cac148b862780f"
  },
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python (.venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
