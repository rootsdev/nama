{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139bee68",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028823c5",
   "metadata": {},
   "source": [
    "# Train a bi-encoder\n",
    "\n",
    "learn name-to-vec encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4d9cdc",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "Hyperparameters\n",
    "* epochs\n",
    "* embedding_dim\n",
    "\n",
    "\n",
    "new data @ 12 epochs:         ??? ??? ??, 189 7, 1011 193, 61728\n",
    "old data @ 12 epochs:         132 124 13,   1 0,  330 197, 56461\n",
    "old data w model @ 12 epochs: 145 110 76,   8 0,  482 246, 48875"
   ]
  },
  {
   "cell_type": "code",
   "id": "e0364890",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T17:50:03.192778Z",
     "start_time": "2024-11-26T17:50:02.836748Z"
    }
   },
   "source": [
    "from collections import Counter\n",
    "import random\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from nama.data.filesystem import download_file_from_s3\n",
    "from nama.data.utils import read_csv\n",
    "from nama.models.biencoder import BiEncoder\n",
    "\n",
    "# from nama.models.tokenizer import get_tokenize_function_and_vocab"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "774de195",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T17:47:15.314335Z",
     "start_time": "2024-11-26T17:47:15.307425Z"
    }
   },
   "source": [
    "given_surname = \"surname\"\n",
    "model_type = 'cecommon+0+aug'\n",
    "\n",
    "checkpoint_path = None  # '../data/models/bi_encoder-given-cecommon+0-0-2.state'\n",
    "# cross-encoder-triplets common-0-augmented.csv = \n",
    "#   cross-encoder-triplets-common + \n",
    "#   cross-encoder-triplets-0 + \n",
    "#   tree-hr-triplets-v1-1000-augmented\n",
    "path_epochs = [\n",
    "    (f\"../data/processed/cross-encoder-triplets-{given_surname}-common-0-augmented.csv\", 3),\n",
    "#     (f\"../data/processed/cross-encoder-triplets-{given_surname}-0.csv\", 6),\n",
    "#     (f\"../data/processed/cross-encoder-triplets-{given_surname}-common-0.csv\", 3),\n",
    "#     (f\"../data/processed/tree-hr-{given_surname}-triplets-v2-1000-augmented.csv.gz\", 6),\n",
    "]\n",
    "\n",
    "# hyperparameters\n",
    "embedding_dim = 256\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "use_amsgrad = False\n",
    "\n",
    "report_size = 10000\n",
    "max_tokens = 10\n",
    "vocab_type = 'f'  # tokenizer based upon training name frequency\n",
    "subword_vocab_size = 2000  # 500, 1000, 1500, 2000\n",
    "nama_bucket = 'nama-data'\n",
    "subwords_path=f\"../data/models/fs-{given_surname}-subword-tokenizer-{subword_vocab_size}{vocab_type}.json\"\n",
    "\n",
    "model_path = f\"../data/models/bi_encoder-{given_surname}-{model_type}\""
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "34474d97",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T17:47:17.081536Z",
     "start_time": "2024-11-26T17:47:17.005142Z"
    }
   },
   "source": [
    "torch.cuda.empty_cache()\n",
    "print(torch.cuda.is_available())\n",
    "print(\"cuda total\", torch.cuda.get_device_properties(0).total_memory)\n",
    "print(\"cuda reserved\", torch.cuda.memory_reserved(0))\n",
    "print(\"cuda allocated\", torch.cuda.memory_allocated(0))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "cuda total 8141471744\n",
      "cuda reserved 0\n",
      "cuda allocated 0\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "5cb453ba",
   "metadata": {},
   "source": [
    "## Combine triplets - only need to run once"
   ]
  },
  {
   "cell_type": "code",
   "id": "adbe8f6b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T17:56:52.340875Z",
     "start_time": "2024-11-26T17:56:51.866184Z"
    }
   },
   "source": [
    "path = download_file_from_s3(f\"s3://fs-nama-data/2023/nama-data/data/processed/cross-encoder-triplets-{given_surname}-0-augmented.csv\")\n",
    "df = read_csv(path)\n",
    "#df2 = read_csv(f\"../data/processed/cross-encoder-triplets-{given_surname}-common.csv\")\n",
    "#df3 = read_csv(f\"../data/processed/tree-hr-{given_surname}-triplets-v2-1000-augmented.csv.gz\")\n",
    "#df = pd.concat([df1, df2, df3])\n",
    "print(df.shape)\n",
    "df.head(5)\n",
    "# df.to_csv(f\"../data/processed/cross-encoder-triplets-{given_surname}-common-0-augmented.csv\", index=False)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading file s3://fs-nama-data/2023/nama-data/data/processed/cross-encoder-triplets-surname-0-augmented.csv from S3: An error occurred (404) when calling the HeadObject operation: Not Found\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/tmp/tmpzecdh15j.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mClientError\u001B[0m                               Traceback (most recent call last)",
      "File \u001B[0;32m~/rootsdev/nama/nama/data/filesystem.py:58\u001B[0m, in \u001B[0;36mdownload_file_from_s3\u001B[0;34m(s3_path)\u001B[0m\n\u001B[1;32m     56\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     57\u001B[0m     \u001B[38;5;66;03m# Download the file from S3 to the temporary file\u001B[39;00m\n\u001B[0;32m---> 58\u001B[0m     \u001B[43ms3\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdownload_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbucket\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtemp_file_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     59\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/rootsdev/nama/.venv/lib/python3.12/site-packages/boto3/s3/inject.py:192\u001B[0m, in \u001B[0;36mdownload_file\u001B[0;34m(self, Bucket, Key, Filename, ExtraArgs, Callback, Config)\u001B[0m\n\u001B[1;32m    191\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m S3Transfer(\u001B[38;5;28mself\u001B[39m, Config) \u001B[38;5;28;01mas\u001B[39;00m transfer:\n\u001B[0;32m--> 192\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtransfer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdownload_file\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    193\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbucket\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mBucket\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    194\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkey\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mKey\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    195\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfilename\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mFilename\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    196\u001B[0m \u001B[43m        \u001B[49m\u001B[43mextra_args\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mExtraArgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    197\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcallback\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mCallback\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    198\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/rootsdev/nama/.venv/lib/python3.12/site-packages/boto3/s3/transfer.py:406\u001B[0m, in \u001B[0;36mS3Transfer.download_file\u001B[0;34m(self, bucket, key, filename, extra_args, callback)\u001B[0m\n\u001B[1;32m    405\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 406\u001B[0m     \u001B[43mfuture\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresult\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    407\u001B[0m \u001B[38;5;66;03m# This is for backwards compatibility where when retries are\u001B[39;00m\n\u001B[1;32m    408\u001B[0m \u001B[38;5;66;03m# exceeded we need to throw the same error from boto3 instead of\u001B[39;00m\n\u001B[1;32m    409\u001B[0m \u001B[38;5;66;03m# s3transfer's built in RetriesExceededError as current users are\u001B[39;00m\n\u001B[1;32m    410\u001B[0m \u001B[38;5;66;03m# catching the boto3 one instead of the s3transfer exception to do\u001B[39;00m\n\u001B[1;32m    411\u001B[0m \u001B[38;5;66;03m# their own retries.\u001B[39;00m\n",
      "File \u001B[0;32m~/rootsdev/nama/.venv/lib/python3.12/site-packages/s3transfer/futures.py:103\u001B[0m, in \u001B[0;36mTransferFuture.result\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     99\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    100\u001B[0m     \u001B[38;5;66;03m# Usually the result() method blocks until the transfer is done,\u001B[39;00m\n\u001B[1;32m    101\u001B[0m     \u001B[38;5;66;03m# however if a KeyboardInterrupt is raised we want want to exit\u001B[39;00m\n\u001B[1;32m    102\u001B[0m     \u001B[38;5;66;03m# out of this and propagate the exception.\u001B[39;00m\n\u001B[0;32m--> 103\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_coordinator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresult\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    104\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/rootsdev/nama/.venv/lib/python3.12/site-packages/s3transfer/futures.py:264\u001B[0m, in \u001B[0;36mTransferCoordinator.result\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    263\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception:\n\u001B[0;32m--> 264\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception\n\u001B[1;32m    265\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_result\n",
      "File \u001B[0;32m~/rootsdev/nama/.venv/lib/python3.12/site-packages/s3transfer/tasks.py:265\u001B[0m, in \u001B[0;36mSubmissionTask._main\u001B[0;34m(self, transfer_future, **kwargs)\u001B[0m\n\u001B[1;32m    263\u001B[0m     \u001B[38;5;66;03m# Call the submit method to start submitting tasks to execute the\u001B[39;00m\n\u001B[1;32m    264\u001B[0m     \u001B[38;5;66;03m# transfer.\u001B[39;00m\n\u001B[0;32m--> 265\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_submit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtransfer_future\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtransfer_future\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    266\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    267\u001B[0m     \u001B[38;5;66;03m# If there was an exception raised during the submission of task\u001B[39;00m\n\u001B[1;32m    268\u001B[0m     \u001B[38;5;66;03m# there is a chance that the final task that signals if a transfer\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    277\u001B[0m \n\u001B[1;32m    278\u001B[0m     \u001B[38;5;66;03m# Set the exception, that caused the process to fail.\u001B[39;00m\n",
      "File \u001B[0;32m~/rootsdev/nama/.venv/lib/python3.12/site-packages/s3transfer/download.py:352\u001B[0m, in \u001B[0;36mDownloadSubmissionTask._submit\u001B[0;34m(self, client, config, osutil, request_executor, io_executor, transfer_future, bandwidth_limiter)\u001B[0m\n\u001B[1;32m    349\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m transfer_future\u001B[38;5;241m.\u001B[39mmeta\u001B[38;5;241m.\u001B[39msize \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    350\u001B[0m     \u001B[38;5;66;03m# If a size was not provided figure out the size for the\u001B[39;00m\n\u001B[1;32m    351\u001B[0m     \u001B[38;5;66;03m# user.\u001B[39;00m\n\u001B[0;32m--> 352\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43mclient\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhead_object\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    353\u001B[0m \u001B[43m        \u001B[49m\u001B[43mBucket\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtransfer_future\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmeta\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall_args\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbucket\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    354\u001B[0m \u001B[43m        \u001B[49m\u001B[43mKey\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtransfer_future\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmeta\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall_args\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    355\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mtransfer_future\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmeta\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall_args\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mextra_args\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    356\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    357\u001B[0m     transfer_future\u001B[38;5;241m.\u001B[39mmeta\u001B[38;5;241m.\u001B[39mprovide_transfer_size(\n\u001B[1;32m    358\u001B[0m         response[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mContentLength\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m    359\u001B[0m     )\n",
      "File \u001B[0;32m~/rootsdev/nama/.venv/lib/python3.12/site-packages/botocore/client.py:569\u001B[0m, in \u001B[0;36mClientCreator._create_api_method.<locals>._api_call\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    568\u001B[0m \u001B[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001B[39;00m\n\u001B[0;32m--> 569\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_api_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43moperation_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/rootsdev/nama/.venv/lib/python3.12/site-packages/botocore/client.py:1023\u001B[0m, in \u001B[0;36mBaseClient._make_api_call\u001B[0;34m(self, operation_name, api_params)\u001B[0m\n\u001B[1;32m   1022\u001B[0m     error_class \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexceptions\u001B[38;5;241m.\u001B[39mfrom_code(error_code)\n\u001B[0;32m-> 1023\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m error_class(parsed_response, operation_name)\n\u001B[1;32m   1024\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[0;31mClientError\u001B[0m: An error occurred (404) when calling the HeadObject operation: Not Found",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[17], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m path \u001B[38;5;241m=\u001B[39m \u001B[43mdownload_file_from_s3\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43ms3://fs-nama-data/2023/nama-data/data/processed/cross-encoder-triplets-\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mgiven_surname\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m-0-augmented.csv\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      2\u001B[0m df \u001B[38;5;241m=\u001B[39m read_csv(path)\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m#df2 = read_csv(f\"../data/processed/cross-encoder-triplets-{given_surname}-common.csv\")\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m#df3 = read_csv(f\"../data/processed/tree-hr-{given_surname}-triplets-v2-1000-augmented.csv.gz\")\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m#df = pd.concat([df1, df2, df3])\u001B[39;00m\n",
      "File \u001B[0;32m~/rootsdev/nama/nama/data/filesystem.py:61\u001B[0m, in \u001B[0;36mdownload_file_from_s3\u001B[0;34m(s3_path)\u001B[0m\n\u001B[1;32m     59\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     60\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mError downloading file \u001B[39m\u001B[38;5;132;01m{\u001B[39;00ms3_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m from S3: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 61\u001B[0m     \u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munlink\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtemp_file_path\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Delete the temporary file\u001B[39;00m\n\u001B[1;32m     62\u001B[0m     temp_file_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     64\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m temp_file_path\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '/tmp/tmpzecdh15j.csv'"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "id": "e7c030b7",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47871b51",
   "metadata": {},
   "source": [
    "### Get tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c28eaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize, tokenizer_vocab = get_tokenize_function_and_vocab(\n",
    "    max_tokens=max_tokens,\n",
    "    subwords_path=subwords_path,\n",
    "    nama_bucket=nama_bucket,\n",
    ")\n",
    "len(tokenizer_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9447cfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize('dallan')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181fba23",
   "metadata": {},
   "source": [
    "### Generate anchor-pos-neg triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdba9648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# array of (anchor_tokens, pos_tokens, neg_tokens, target_margin)\n",
    "def generate_training_data(train_triplets_df):\n",
    "    all_data = []\n",
    "    for anchor, pos, neg, pos_score, neg_score in tqdm(zip(\n",
    "        train_triplets_df['anchor'],\n",
    "        train_triplets_df['positive'],\n",
    "        train_triplets_df['negative'],\n",
    "        train_triplets_df['positive_score'],\n",
    "        train_triplets_df['negative_score'],\n",
    "    ), mininterval=2):\n",
    "        anchor_tokens = tokenize(anchor)\n",
    "        pos_tokens = tokenize(pos)\n",
    "        neg_tokens = tokenize(neg)\n",
    "        target_margin = pos_score - neg_score\n",
    "        # anchor, positive, hard-negative\n",
    "        all_data.append((\n",
    "            anchor_tokens,\n",
    "            pos_tokens,\n",
    "            neg_tokens,\n",
    "            target_margin,\n",
    "        ))\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b5e413",
   "metadata": {},
   "source": [
    "## Train bi-encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbecdb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(anchors, positives, negatives, labels):\n",
    "    # anchor_pos_sim = (anchors * positives).sum(dim=-1)\n",
    "    # anchor_neg_sim = (anchors * negatives).sum(dim=-1)\n",
    "    anchor_pos_sim = F.cosine_similarity(anchors, positives, dim=-1)\n",
    "    anchor_neg_sim = F.cosine_similarity(anchors, negatives, dim=-1)\n",
    "    margin_pred = anchor_pos_sim - anchor_neg_sim\n",
    "    return F.mse_loss(margin_pred, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125b6c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train(model, train_loader, val_loader, loss_fn, optimizer, num_epochs, model_path, verbose=True):\n",
    "    for epoch in range(num_epochs):\n",
    "        # make sure gradient tracking is on\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "\n",
    "        for ix, data in enumerate(train_loader):\n",
    "            # get batch\n",
    "            anchors, positives, negatives, target_margins = data\n",
    "\n",
    "            # zero gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            anchor_embeddings = model(anchors)  # Shape: (batch_size, embedding_dim)\n",
    "            pos_embeddings = model(positives)  # Shape: (batch_size, embedding_dim)\n",
    "            neg_embeddings = model(negatives)  # Shape: (batch_size, embedding_dim)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = loss_fn(anchor_embeddings, pos_embeddings, neg_embeddings, target_margins)\n",
    "\n",
    "            # Backward pass and optimization step\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Calculate loss and report\n",
    "            if verbose:\n",
    "                running_loss += loss.item()\n",
    "                if ix % report_size == report_size - 1:\n",
    "                    avg_loss = running_loss / report_size  # loss per batch\n",
    "                    print(f\"Epoch {epoch} batch {ix} loss {avg_loss}\")\n",
    "                    running_loss = 0\n",
    "\n",
    "        # set model to evaluation mode\n",
    "        model.eval()\n",
    "\n",
    "        # disable gradient computation\n",
    "        running_loss = 0\n",
    "        num_val_batches = 0\n",
    "        with torch.no_grad():\n",
    "            for data in val_loader:\n",
    "                anchors, positives, negatives, target_margins = data\n",
    "                anchor_embeddings = model(anchors)  # Shape: (batch_size, embedding_dim)\n",
    "                pos_embeddings = model(positives)  # Shape: (batch_size, embedding_dim)\n",
    "                neg_embeddings = model(negatives)  # Shape: (batch_size, embedding_dim)\n",
    "                loss = loss_fn(anchor_embeddings, pos_embeddings, neg_embeddings, target_margins)\n",
    "                running_loss += loss.item()  \n",
    "                num_val_batches += 1\n",
    "\n",
    "        # calculate average validation loss\n",
    "        val_loss = running_loss / num_val_batches\n",
    "        if verbose:\n",
    "            print(f\"VALIDATION: Epoch {epoch} loss {val_loss}\")\n",
    "        # save model state + model\n",
    "        epoch_model_path = f\"{model_path}-{epoch}\"\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "        }, epoch_model_path+\".state\")\n",
    "        torch.save(model, epoch_model_path+\".pth\")\n",
    "        \n",
    "    # return final epoch validation loss\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fefd66",
   "metadata": {},
   "source": [
    "## Hyperparameter search"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5b2c2b4e",
   "metadata": {},
   "source": [
    "def hyperopt_objective_function(train_data, \n",
    "                                val_data, \n",
    "                                vocab_size,\n",
    "                                max_tokens,\n",
    "                                pad_token,\n",
    "                                verbose=True,\n",
    "                               ):\n",
    "    \n",
    "    def objective(config):\n",
    "        learning_rate = config['learning_rate']\n",
    "        batch_size = config['batch_size']\n",
    "        embedding_dim = config['embedding_dim']\n",
    "        num_epochs = config['num_epochs']\n",
    "        \n",
    "        if verbose:\n",
    "            print('train', config)\n",
    "        \n",
    "        # Create an instance of the bi-encoder model\n",
    "        model = BiEncoder(embedding_dim, vocab_size, max_tokens, pad_token)\n",
    "        # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        # model.to(device)\n",
    "\n",
    "        # Define the optimizer\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=learning_rate, amsgrad=use_amsgrad)\n",
    "\n",
    "        # Create data loader\n",
    "        train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True) \n",
    "\n",
    "        val_loss = train(model, train_loader, val_loader, loss_fn, optimizer, num_epochs, verbose=False)\n",
    "        if verbose:\n",
    "            print('val_loss', val_loss)\n",
    "        \n",
    "        return {\n",
    "            'status': STATUS_OK,\n",
    "            'loss': val_loss,\n",
    "            'config': config,            \n",
    "        }\n",
    "\n",
    "    return objective"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5ccb3a65",
   "metadata": {},
   "source": [
    "# HyperOpt search space\n",
    "search_space = {\n",
    "    \"learning_rate\": hp.loguniform('learning_rate', math.log(1e-4), math.log(1e-2)),\n",
    "    \"batch_size\": hp.choice('batch_size', [8,16,32,64]),\n",
    "    \"embedding_dim\": hp.choice('embedding_dim', [8,16,32,64]),\n",
    "    \"num_epochs\": hp.choice('num_epochs', [5,10,20,40]),\n",
    "}\n",
    "objective = hyperopt_objective_function(train_data=train_data,\n",
    "                                        val_data=val_data,\n",
    "                                        vocab_size=len(tokenizer_vocab),\n",
    "                                        max_tokens=max_tokens,\n",
    "                                        pad_token=tokenizer_vocab['[PAD]'],\n",
    "                                        verbose=True,\n",
    "                                       )\n",
    "trials = Trials()\n",
    "\n",
    "# minimize the objective over the space\n",
    "best = fmin(objective, \n",
    "            search_space, \n",
    "            algo=tpe.suggest, \n",
    "            trials=trials,\n",
    "            max_evals=50)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a278ccb0",
   "metadata": {},
   "source": [
    "print(\"best\", best)\n",
    "print(\"results\", trials.results) "
   ]
  },
  {
   "cell_type": "raw",
   "id": "4a735dc7",
   "metadata": {},
   "source": [
    "batch_size = best_result.config['batch_size']\n",
    "learning_rate = best_result.config['learning_rate']\n",
    "embedding_dim = best_result.config['embedding_dim']\n",
    "num_epochs = best_result.config['num_epochs']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba77e31",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0b81bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the bi-encoder model\n",
    "model = BiEncoder(embedding_dim, len(tokenizer_vocab), max_tokens, tokenizer_vocab['[PAD]'])\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate, amsgrad=use_amsgrad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f926160f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_collate_fn(batch):\n",
    "    # Transpose the batch (list of tuples) to a tuple of lists\n",
    "    transposed_batch = list(zip(*batch))\n",
    "    # Convert each list in the tuple to a tensor\n",
    "    tensor_batch = tuple(torch.tensor(x) for x in transposed_batch)\n",
    "    return tensor_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9349c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if checkpoint_path:\n",
    "    print(checkpoint_path)\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276d6206",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for ix, (path, num_epochs) in enumerate(path_epochs):\n",
    "    print(path, num_epochs)\n",
    "    train_triplets_df = read_csv(path)\n",
    "    all_data = generate_training_data(train_triplets_df)\n",
    "    del train_triplets_df\n",
    "    train_data, val_data = train_test_split(all_data, test_size=0.01, random_state=42)\n",
    "    del all_data\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=my_collate_fn)\n",
    "    val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True, collate_fn=my_collate_fn)\n",
    "    train(model, train_loader, val_loader, loss_fn, optimizer, num_epochs, f\"{model_path}-{ix}\")\n",
    "    del train_loader\n",
    "    del val_loader\n",
    "    del train_data\n",
    "    del val_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5985e2a",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47966391",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cc600c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "torch.save(model, model_path+\".pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996366e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
